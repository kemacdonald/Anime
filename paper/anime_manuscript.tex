\documentclass[english,floatsintext,man]{apa6}

\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}

% Table formatting
\usepackage{longtable, booktabs}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\begin{center}\begin{threeparttable}}
%   {\end{threeparttable}\end{center}\end{landscape}}

\newenvironment{lltable}
  {\begin{landscape}\begin{center}\begin{ThreePartTable}}
  {\end{ThreePartTable}\end{center}\end{landscape}}




% The following enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand\getlongtablewidth{%
 \begingroup
  \ifcsname LT@\roman{LT@tables}\endcsname
  \global\longtablewidth=0pt
  \renewcommand\LT@entry[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}%
  \@nameuse{LT@\roman{LT@tables}}%
  \fi
\endgroup}


\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            pdfauthor={},
            pdftitle={Moos as cues: Two-year-olds expect one-to-one mappings in a non-linguistic and non-communicative domain},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=black,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls

\setlength{\parindent}{0pt}
%\setlength{\parskip}{0pt plus 0pt minus 0pt}

\setlength{\emergencystretch}{3em}  % prevent overfull lines

\ifxetex
  \usepackage{polyglossia}
  \setmainlanguage{}
\else
  \usepackage[english]{babel}
\fi

% Manuscript styling
\captionsetup{font=singlespacing,justification=justified}
\usepackage{csquotes}
\usepackage{upgreek}

 % Line numbering
  \usepackage{lineno}
  \linenumbers


\usepackage{tikz} % Variable definition to generate author note

% fix for \tightlist problem in pandoc 1.14
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% Essential manuscript parts
  \title{Moos as cues: Two-year-olds expect one-to-one mappings in a
non-linguistic and non-communicative domain}

  \shorttitle{Moos as cues}


  \author{Kyle MacDonald\textsuperscript{1}, Ricardo A. H. Bion\textsuperscript{1}, \& Anne Fernald\textsuperscript{1}}

  \def\affdep{{"", "", ""}}%
  \def\affcity{{"", "", ""}}%

  \affiliation{
    \vspace{0.5cm}
          \textsuperscript{1} Stanford University  }

 % If no author_note is defined give only author information if available
      \newcounter{author}
                              \authornote{
            Correspondence concerning this article should be addressed to Kyle MacDonald, 450 Serra Mall, Stanford, CA 94306. E-mail: \href{mailto:kylem4@stanford.edu}{\nolinkurl{kylem4@stanford.edu}}
          }
                                                

  \abstract{When hearing a novel word, children tend to select a novel object rather
than a familiar one: A bias that can facilitate word learning in
ambiguous labeling contexts. But what cognitive processes explain this
behavior? Different theoretical accounts have proposed explanations
based on (a) constraints on the structure of the lexicon, (b) pragmatic
inferences about speakers' communicative intent, and (c) domain-general
inferences. Here, we ask whether disambiguation behaviors emerge in a
domain that is non-linguistic and non-communicative, but in which there
are strong regularities: animal vocalizations. Using real-time
processing measures, we show that two-year-olds identify familiar
animals based on their vocalizations, though not as fast as when hearing
their names or their onomatopeic labels. We then show that children tend
to look at an unfamiliar animal when hearing a novel animal vocalization
or novel animal name, responding with similar efficiency to both cues.
In Experiment 2, we replicate the key finding that children can
disambiguate novel animal vocalizations, but find that disambiguation
does not necessarily lead to retention measured at a longer timescale.
These results are consistent with an account of disambiguation arising
from domain-general inferences and contribute to theories about whether
language is a special stimulus.}
  \keywords{disambiguation, mutual exclusivity, environmental sounds, retention,
word learning \\

    \indent Word count: 7946
  }





\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{example}{Example}
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}

\maketitle

\setcounter{secnumdepth}{0}



\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Words are a core feature of natural languages, providing a link between
external communicative signals and internal lexical concepts. A central
property of words is that they tend to correspond to a single meaning,
i.e., one-to-one mappings. This regularity can help novice word learners
disambiguate reference in complex labeling contexts with multiple
possible referents.\footnote{This problem is a simplified version of
  Quine's \textit{indeterminacy of reference} (Quine, 1960): That there
  are many possible meanings of a word (\enquote{Gavigai}) that include
  the referent (\enquote{Rabbit}) in their extension, e.g.,
  \enquote{white,} \enquote{rabbit,} \enquote{dinner.} Quine's broader
  philosophical point was that different meanings (\enquote{rabbit} and
  \enquote{undetached rabbit parts}) could be extensionally identical
  and thus impossible to tease apart.} Even young children behave as if
they expect words to map onto single concepts, tending to select a novel
object when they hear a novel name, a behavior know as the
\enquote{disambiguation} effect. But what cognitive processes underly
this behavior? And is disambiguation limited to linguistic or
communicative domains?

Consider that children's daily experience consists of concurrent input
from multiple sensory modalities, creating a scenario where learners
could learn several multimodal associations. For example, when playing
with their pet, a child could learn to link the physical features and
actions of a dog with its barking sound or with the onomatopoeic word
resembling its characteristic vocalization, such as woof-woof.
Eventually, the child will learn to associate the word \enquote{dog}
with these other features. That is, just as familiar object labels are
consistently associated with particular referents, animal vocalizations
and onomatopoeic words for vocalizations also provide consistent
associations between an auditory stimulus and an object in the visual
world. Unlike words, however, animal vocalizations are not a linguistic
or communicative stimulus. Together, these features make animal
vocalizations a particularly interesting stimulus for testing theories
of children's disambiguation behavior and asking whether children
respond to words as a unique kind of auditory stimulus. In work reported
here, we compare young children's efficiency in using words,
onomatopoeia, and animal vocalizations as cues to identifying animals in
a visual scene. We also ask whether children will map a novel animal
vocalization to an unfamiliar animal, showing disambiguation behavior in
a non-linguistic and non-communicative domain.

The question of whether words are a special kind of stimulus has a rich
tradition in the cognitive sciences. Several studies have found
advantages for speech sounds over tones in object individuation and
categorization in young infants (Fulkerson \& Waxman, 2007; Xu, 2002).
Focusing on associations between objects and sounds, objects and tones,
or objects and gestures, several studies found that younger infants
accept several different forms as potential object labels, but that
older infants are more discriminating and show a preference for words
(Namy \& Waxman, 1998; Woodward \& Hoyne, 1999). Another line of
research found that infants prefer to hear spoken words over some
non-linguistic analogs (Vouloumanos \& Werker, 2004, 2007a, 2007b) and
that the neonate brain responds differently to speech as compared to
backward speech (Pena et al., 2003). These studies found advantages for
speech over non-linguistic analogs in categorization, individuation,
crossmodal association, and speech preferences. However, they all used
arbitrary, non-linguistic cues (e.g., tones) that are not consistently
associated with objects in children's everyday environments.

Other research has approached the question of whether speech is special
from a different perspective, comparing how people process spoken words
as compared to non-arbitrary environmental sounds, such as animal
vocalizations (e.g., cat meowing) or the sounds produced by inanimate
objects (e.g., car starting). Studies with adults have found
similarities and differences in both behavioral and neural responses to
cross-modal semantic associations between words and environmental
sounds. For example, in a picture detection task, Chen and Spence (2011)
found a facilitation effect for environmental sounds but not for words
when the onset of the auditory stimulus preceded the image by
approximately 350 ms. In a follow-up study, Chen and Spence (2013)
presented environmental sounds and words across a wider range of time
intervals before the image onset. They found that both naturalistic
sounds and spoken words resulted in cross-modal priming, but that the
effect of spoken words required more time between the auditory and
visual stimuli as compared to the naturalistic sounds. These data are
consistent with a differential processing account: that the recognition
of environmental sounds is faster because words must also be processed
at a lexical stage before accessing semantic representations, whereas
environmental sounds activate semantic representations directly.

In contrast, other studies have found an advantage for the processing of
lexical items compared to environmental sounds. For example, in a
sound-to-picture matching task, Lupyan and Thompson-Schill (2012) showed
that words (\enquote{cat}) lead to faster and more accurate object
recognition as compared to either nonverbal cues (the sound of a cat
meowing) or lexicalized versions of the nonverbal cue (the word
\enquote{meowing}). Recent work by Edmiston and Lupyan (2015) followed
up on this result by manipulating the congruency between the
environmental sounds and their corresponding images within the same
basic-level category (e.g., pairing the sound of an acoustic guitar with
an image of either an acoustic or an electric guitar). Adults were
faster to identify a congruent sound-image pair, suggesting that the
environmental sound carried additional information about the specific
type of object generating that sound. Critically, adults were fastest to
identify the images after hearing their labels, which Edmiston and
Lupyan (2015) argue is driven by words evoking more abstract category
representations (decoupled from surface-level features of a particular
category member) that facilitate rapid category disambiguation.

In an ERP study with adults, Cummings et al. (2006) found that largely
overlapping neural networks processed verbal and non-verbal meaningful
sounds. In another study focusing on three different sound types that
varied in arbitrariness, Hashimoto et al. (2006) found different neural
mechanisms for the processing of animal names and vocalizations, with
onomatopoeic words activating both areas. Recent work by Uddin et al.
(2018a) showed that adults were able to use prior sentential context to
facilitate recognition of both speech and environmental, also finding
evidence of parallel neural responses using EEG measures in a similar
paradigm (Uddin et al., 2018b). Because research on environmental sounds
is relatively new, it is challenging to reconcile these discrepant
findings. Variations in tasks or timing of stimuli could influence
results, and different theoretical commitments can lead to different
interpretations. For example, environmental sounds are often treated as
encompassing both the sounds of living and human-made objects (e.g., cow
mooing, bell ringing) despite evidence that these sounds are interpreted
differently by the adult brain (Murray, Camen, Andino, Bovet, \& Clarke,
2006). Nevertheless, this line of research provides promising new ways
to examine the question of whether language emerges from the interaction
of domain-general cognitive processes or domain-specific mechanism
(Bates, MacWhinney, \& others, 1989). For example, recent research
comparing the processing of speech and non-speech sounds is leading to
new insights relevant to autism, developmental language impairment, and
cochlear implants (Cummings \& Ceponiene, 2010; McCleery et al., 2010).

From a developmental perspective, it is also important to understand
whether children's processing of words and non-arbitrary, non-linguistic
sounds changes as a function of experience with these cues. However, few
studies have examined this question. Using a preferential-looking
paradigm, Cummings, Saygin, Bates, and Dick (2009) found that 15- and
25-mo-olds could use words and environmental sounds to guide their
attention to familiar objects, becoming more efficient with both cues as
they get older. Vouloumanos, Druhen, Hauser, and Huizink (2009) found
that 5-month-olds can match some animals to the vocalizations they
produce. And studies with children with autism and developmental
language impairment found more severe deficits for the processing of
words than environmental sounds (Cummings \& Ceponiene, 2010; McCleery
et al., 2010).

In work reported here, we build on this developmental approach, using
the looking-while-listening paradigm (Fernald, Zangl, Portillo, \&
Marchman, 2008) to compare children's real-time processing of different
auditory stimuli that are consistently associated with familiar animals
but vary in arbitrariness. First, we ask whether 32-month-olds can use
animal vocalizations (e.g., dog barking), onomatopoeic sounds (e.g.,
bow-wow), and familiar animal names (e.g., dog) to identify familiar
animals. By using real-time processing measures, we can measure the
timecourse of recognition and ask whether these three sounds are equally
effective as acoustic cues in guiding children's attention to a
particular animal in the visual scene. The use of looking to visual
stimuli, rather than object-choice responses, reduces the task demands
of procedures requiring more complex responses such as reaching or
pointing and yield continuous rather than categorical measures of
attention on every trial, capturing differences in processing that might
not be detected by offline tasks. This feature is important for
answering questions about whether speech processing would be different
from the other auditory stimuli in our task.

A second major goal of this research is to investigate how young
children learn to disambiguate non-linguistic sounds, linking them to
animate objects in the visual scene. Would two-year-olds map a novel
animal vocalization to an unfamiliar animal? Typically, word learning is
portrayed as an intractable challenge, while associating animals with
the sounds they produce might appear trivial. The acoustic structure of
vocalizations is influenced by the size and shape of the vocal tract and
other physical features, linking sounds to their source in a
non-arbitrary way. And the fact that many animal vocalizations are
accompanied by synchronous physical movements might provide children
with additional non-arbitrary cues to the source of the sound. Even in
the absence of additional visual cues, it is often possible to pinpoint
the source of a sound with reasonable accuracy. In contrast, because the
acoustic structure of a word is in most cases arbitrary concerning
potential referents, and it is produced by a speaker and not by the
object itself, learning to associate speech sounds with objects presents
a complex problem of induction (Markman, 1991).

One solution to the word-learning puzzle is to reduce referential
ambiguity in the learning task by constraining the possible meanings of
a novel word. One widely-studied constraint is that children expect
words to map onto a single concept (Diesendruck \& Markson, 2001;
Markman, 1991). Evidence for this bias comes from experiments in which
children hear a novel label in the presence of a novel object and one or
more familiar objects and tend to select the novel object as the
referent for the novel word. The debate about the origins, scope, and
generality of this constraint has focused on whether this behavior
provides evidence for a lexical constraint or whether it results instead
from pragmatic inferences about a speaker's communicative intent. Under
a lexical account, the constraint emerges via a \enquote{domain specific
mechanism specific to word leaning} (Marchena, Eigsti, Worek, Ono, \&
Snedeker, 2011; Scofield \& Behrend, 2007). In contrast, pragmatic
accounts propose that disambiguation behaviors should arise in any
communicative context, reflecting assumptions that speakers are
cooperative and use conventional names to refer to familiar objects
(Bloom, 2002; Clark, 1990). A third possibility is that the bias toward
one-to-one mappings reflects general tendencies to find simple
regularities in complex domains, a perspective embraced by recent
computational approaches to word learning (Frank, Goodman, \& Tenenbaum,
2009; McMurray, Horst, \& Samuelson, 2012; Regier, 2003).

Thus, lexical and pragmatic accounts of the scope of children's
disambiguation behavior predict that one-to-one biases are unique to
word learning or that they generalize to communicative acts more
broadly, while domain-general accounts predict that they would apply to
any domain in which consistent one-to-one mappings are observed. To
explore the possibility that one-to-one biases in sound-object mappings
are not limited to interpreting communicative acts, we investigated
whether children would show disambiguation behavior in a domain that is
neither linguistic nor communicative, but in which consistent
associations are observed between objects and auditory cues: animal
vocalizations.

The third goal of this research is to explore the link between
children's disambiguation behavior and their retention of different
types of sound-object associations. Recent theoretical accounts of word
learning have emphasized the conceptual distinction between
situation-time behaviors (figuring out the referent of a word) and
developmental-time processes (slowly forming stable mappings between
words and concepts) (McMurray et al., 2012). Moreover, empirical work
suggests that these two constructs should not be conflated. For example,
Horst and Samuelson (2008) found that children showed little evidence of
remembering the names of novel objects they had previously identified
via disambiguation after just a five-minute delay. And, using a
looking-time task, Bion, Borovsky, and Fernald (2013) replicated these
findings in a study with 18-, 24-, and 30-month-old infants using
looking time measures. In Experiment 2, we explore this question and
include measures of retention to ask whether successful disambiguation
leads to recall of novel animal-vocalization associations over a short
delay.

\hypertarget{experiment-1}{%
\section{Experiment 1}\label{experiment-1}}

Experiment 1 asks two questions. First, can 32-month-olds use familiar
animal names (e.g., dog), onomatopoeic words (e.g., bow-wow), and animal
vocalizations (e.g., dog barking) to identify familiar animals? These
three sound types differ in arbitrariness along a continuum, with speech
as the most arbitrary and vocalizations as the least arbitrary cue. We
ask whether these sounds are equally effective as acoustic cues in
guiding children's attention to animals in a visual scene. Children will
hear a sound cue while looking at images of two familiar animals, one
that matches and one that does not match the cue, and we will compare
children's looking to the matching animal when hearing the target sound.

One of three patterns of results is most likely to emerge: The first is
that children are faster to identify animal names than onomatopoeic
words, and faster to identify onomatopoeic sounds than animal
vocalizations. This pattern of results could be predicted by
computational models that use frequency as a crucial determinant of
processing speed (e.g., McMurray et al. (2012)). That is, children in
urban environments are more likely to hear the names of animals than
their vocalizations, resulting in more practice interpreting speech
(high SES children might hear thousands of words daily, but not nearly
as many animal sounds). This pattern of results could also be predicted
by developmental accounts privilege language cues early development --
either for getting children's attention (Vouloumanos \& Werker, 2004,
2007a, 2007b) or for the fact that words refer to objects directly
(Waxman \& Gelman, 2009) and might evoke more category-diagnostic
features compared to environmental sounds (Edmiston \& Lupyan, 2015).

The second possible pattern of results is that children are faster to
identify animal vocalizations than onomatopoeic sounds, and faster to
identify onomatopoeic sounds than animal names. This pattern of results
could be predicted by accounts that propose that non-arbitrary sounds
link directly to semantic representations, while words first activate
lexical representations before reaching semantics (Chen \& Spence, 2011,
2013). The fact that children have experience interpreting environmental
sounds (e.g., balls bouncing, things falling) before learning to
interpret speech referentially could also predict an advantage for
environmental sounds. A third possibility is that children are equally
efficient in exploiting these three sound types to guide their attention
to a familiar animal. This pattern of results would parallel that of
previous studies that showed little difference in the processing of
environmental sounds and words (Cummings et al., 2009).

Our second question is whether two-year-olds make similar inferences
when mapping a novel name and a novel animal vocalization to an
unfamiliar animal. In our task, children will hear a novel animal name
or vocalization (instead of a familiar one), while looking at the
picture of a familiar and a novel animal (instead of two familiar
objects). We compare children's proportion of looking to the novel
animal when hearing one of these two sound cues.

We predict that children will look at a novel animal when hearing a
novel name. When hearing a novel vocalization, one of two patterns of
results is most likely to emerge: Children show no looking preference.
This result would be consistent with lexical accounts that predict
disambiguation only within the domain of linguistic stimuli, or by
pragmatic accounts that predict disambiguation only within communicative
contexts. In contrast, children could look at a novel animal when
hearing a novel vocalization, with comparable performance across these
two cues. This result would be consistent with accounts that propose
that disambiguation biases emerge from domain-general mechanisms that
look for regularities in the input.

\hypertarget{method}{%
\subsection{Method}\label{method}}

\hypertarget{participants}{%
\subsubsection{Participants}\label{participants}}

Participants were 23 32-month-old children (M=31.10; range = 30,32, 12
girls. All were reported by parents to be typically developing and from
families where English was the dominant language. Two participants were
excluded due to fussiness. Children were from mid/high-SES families.

\begin{figure}[tb]

{\centering \includegraphics[width=0.7\linewidth]{anime_manuscript_files/figure-latex/stimuli-e1-1} 

}

\caption{Trial types in Experiments 1 and 2 organized by type of cue: Familiar vs. Novel. The target animal for each trial type is on the left.}\label{fig:stimuli-e1}
\end{figure}

\hypertarget{visual-stimuli}{%
\subsubsection{Visual stimuli}\label{visual-stimuli}}

The visual stimuli included pictures of four Familiar animals (horse,
dog, cow, sheep) and two Novel animals (pangolin, tapir). According to
parental report, the familiar animals were known by all children.
Parents also reported that the novel animals were completely unfamiliar.
Each animal picture was centered on a grey background in a 640 x 480
pixel space

\hypertarget{auditory-stimuli}{%
\subsubsection{Auditory stimuli}\label{auditory-stimuli}}

The auditory stimuli consisted of sounds that were either familiar or
novel to 32-month-olds. Figure 1 shows the different sound types. The
familiar trials consisted of one of three different sounds: names
(horse, dog, cow, and sheep), onomatopoeic words (neigh, woof-woof, moo
and baa), and vocalizations (horse neighing, dog barking, cow mooing and
sheep baaing). The novel sounds were used in disambiguation trials and
consisted of one of two types of sounds: names (capa, nadu) and
vocalizations (rhino grunting, gorilla snorting).

Trials in which the auditory cue was a familiar or novel animal name
(e.g., Where's the dog?) or a familiar or novel lexical sound (Which one
goes woof-woof?) began with a brief carrier frame. The duration of the
target cue was 810 ms for lexical sounds and 750 ms for animal names.
The intensity of the phrases was normalized using Praat speech analysis
software (Boersma, 2002).

Trials animal vocalizations began with a single word, used to draw
children's attention (e.g., Look! \enquote{dog barking}). Familiar
animal vocalizations were selected based on prototypicality. After
selecting at least three vocalizations for each familiar animal, the
authors voted on the one that we thought would be most easily recognized
by children. Choosing the novel animal vocalizations was more
challenging. A group of research assistants selected from different
websites several vocalizations that they judged as unfamiliar. From
these vocalizations, we selected two (i.e., rhino grunting and gorilla
snorting) that we judged were equally likely to be produced by the six
familiar and two novel animals based on their size and vocal tract
characteristics.

We counterbalanced the vocalizations that were paired with the two novel
animals, to control for the possibility that children judged one of the
two novel animals as more likely to produce one of the novel
vocalizations. All children were reported by parents to have had no
exposure to the novel animal's natural vocalizations. The duration of
the target animal vocalizations was 2000 ms.

\hypertarget{familiarization-books}{%
\subsubsection{Familiarization books}\label{familiarization-books}}

Since we were working with children from mid/high-SES families growing
up in an urban environment, we were concerned that they would not be
familiar with many animal vocalizations or onomatopoeic words. So to
ensure that all children had at least some experience with the familiar
animals and auditory cues in our study, we sent parents two children's
books, both titled Sounds on the Farm, a week before their visit.
Parents were instructed to share each book with their child for 5 to 10
min on at least three days before the experiment. The first book
consisted of colorful pictures of each familiar animal and text designed
to prompt parents to produce each animal's lexical sound (e.g., Wow,
look at all those cows! This cow says moo, moo!). To give children
exposure to the natural animal vocalizations, we used a Hear and There
book, which contained buttons that children could press to hear the
actual noise that each animal produces.

We thought this design decision was important for two reasons. First, it
would provide more confidence that differences in children's performance
within familiar trials would be related to processing speed and not
driven by lack of exposure to one of the sound types. Second, the
association between familiar vocalizations and animals is a prerequisite
for success on disambiguation, a key question for our experiments.

\hypertarget{procedure}{%
\subsubsection{Procedure}\label{procedure}}

We used the looking-while-listening (LWL) procedure (see Fernald et al.,
2008) to measure differences in processing familiar sounds. Previous
studies have shown that even when objects are reported by parents as
familiar to their children, or when children are at ceiling in offline
reaching tasks, these real-time processing measures can capture
meaningful differences in processing. These differences correlate to
properties of the sound stimuli (e.g., word-frequency) and different
aspects of the child's experience (e.g., their age, socioeconomic
status, amount of parental talk). Looking-time measures have also been
used in disambiguation tasks with children from different ages,
capturing differences in accuracy that relate to children's age and
vocabulary size (Bion et al., 2013).

On each trial, a pair of pictures was presented on the screen for
approximately 4 s, with the auditory stimuli starting after 2 s,
followed by 1 s of silence. As seen in Figure 1, we have two main trials
types, familiar and disambiguation, paralleling our original two
research questions on children's processing of familiar and novel
auditory cues.

There were three types of familiar trials: name, onomatopoeic word, and
vocalization. On 8 Name trials, each familiar animal served as the
target twice and was paired once with another familiar animal and once
with a novel animal. On 8 Onomatopoeic-word trials, each familiar animal
served as the target twice. On 16 Vocalization trials, each familiar
animal served as the target four times, paired twice with another
familiar animal and twice with a novel animal. These three familiar
sound types should allow us to answer our first research question:
whether names, onomatopoeic words, or animal vocalizations, are equally
effective as acoustic cues in guiding children's attention to animals in
a visual scene.

There were two types of disambiguation trials: names and vocalizations.
On 6 Name trials, each novel animal was labeled three times with a novel
animal name, always paired with a familiar animal. On 8 Vocalization
trials, each novel animal vocalization served as the target four times
and was paired with each familiar animal once. These two sound types
should allow us to answer our second research question: whether
two-year-olds make similar inferences when mapping a novel name and a
novel animal vocalization to an unfamiliar animal.

There were two different visits. The familiar and disambiguation trials
with animal names and onomatopoeic words were administered during visit
one. The Familiar and Disambiguation Trials with the animal
vocalizations were administered during their second visit. We
administered the animal vocalizations on the second visit to allow
children to become familiar with the procedure and to give parents
additional time to use the familiarization books.

During each visit, five filler trials were interspersed throughout to
add variety and maintain children's attention. Pairings of the novel
animal and name and side of presentation of target animals were
counterbalanced across participants. Caregivers wore darkened sunglasses
so that they could not see the pictures and influence infants' looking
throughout the 5-min procedure.

\hypertarget{measures-of-processing-efficiency}{%
\subsubsection{Measures of processing
efficiency}\label{measures-of-processing-efficiency}}

Participants' eye movements were video-recorded and coded with a
precision of 33 ms by observers who were blind to trial type. For 25\%
of the subjects, two measures of inter-observer reliability were
assessed: the proportion of frames on which two coders agreed (98\%) and
timing of shifts in gaze, ignoring steady-state fixations (94\%).

\textbf{Accuracy:} On those trials in which the infant was fixating a
picture at the onset of the speech stimulus, accuracy was computed by
dividing the time looking to the target object by the time looking to
both target and distracter, from 300 to 2500 ms from the onset of the
target sound. We chose this window because shifts to the target
occurring before 300 ms were likely initiated before the onset of the
noun and the animal vocalizations were on average two seconds in
duration. We used a single window for all trial types and computed mean
accuracy for each participant on each trial type.

\textbf{Reaction time:} We calculated reaction time (RT) on those trials
on which participants were looking at the distractor animal at the
beginning of the sound. RT on each trial was the latency of the first
shift to the correct animal within a 300- to 1,800-ms window from sound
onset, as typically done in studies using this procedure (Fernald et
al., 2008).

\hypertarget{results-and-discussion}{%
\subsection{Results and discussion}\label{results-and-discussion}}

\hypertarget{using-familiar-animal-names-onomatopeic-sounds-and-vocalizations-to-identify-familiar-animals}{%
\subsubsection{Using familiar animal names, onomatopeic sounds, and
vocalizations to identify familiar
animals:}\label{using-familiar-animal-names-onomatopeic-sounds-and-vocalizations-to-identify-familiar-animals}}

Our first question is whether a familiar animal name, onomatopoeic word,
and animal vocalization are equally effective in guiding children's
attention to an animal in the visual scene. Figure 2A shows children's
looking behavior over time on the LWL procedure (Fernald, Pinto,
Swingley, Weinbergy, \& McRoberts, 1998). To capture children's speed of
processing, we show children's responses on trials in which they start
looking at the wrong animal. From sound onset onward, we show the mean
proportion of trials in which children were looking at the correct
picture, every 33ms, with different lines representing children's
responses on Name trials (black), Onomatopoeic-word trials (light grey),
and Vocalization trials (dark grey). The y-axis shows the mean
proportion of trials on which children were looking at the correct
animal. The x-axis represents time from sound onset in milliseconds.
Around 750ms from sound onset there is already a substantial difference
in the mean proportion of trials in which children are looking at the
correct animal depending on whether they heard an animal name or an
animal vocalization. Children are looking at the correct animal in a
greater proportion of trials when they hear a familiar animal name, as
compared to when they hear an animal vocalization, with performance on
trials with onomatopoeic words falling between the two.

To quantify these differences, we fit Bayesian mixed-effects regression
models using the \texttt{rstanarm} (Gabry \& Goodrich, 2016) package in
R (3.4.1, R Core Team, 2017)\footnote{We, furthermore, used the
  R-packages \emph{here} (0.1, MÃ¼ller, 2017), \emph{knitr} (1.20, Xie,
  2015), \emph{papaja} (0.1.0.9492, Aust \& Barth, 2017), and
  \emph{tidyverse} (1.2.1, Wickham, 2017).}. The mixed-effects approach
allowed us to model the nested structure in our data by including random
intercepts for each participant and item and a random slope for each
item. We used Bayesian methods to quantify support in favor of null
hypotheses of interest -- in this case, the absence of a difference in
real-time processing across the different familiar cue types. To
communicate the uncertainty in our estimates, we report the 95\% Highest
Density Interval (HDI) around the point estimates of the group means and
the difference in means. The HDI provides a range of plausible parameter
values given the data and the model. All analysis code is available in
the online repository for this project:
\url{https://github.com/kemacdonald/anime}.

We computed reaction time (RT) as the mean time it took them to shift to
the correct picture on trials in which they were looking at the wrong
picture at sound onset for the three cue types. To make RTs more
suitable for modeling on a linear scale, we analyzed responses in log
space using a logistic transformation, with the final model was
specified as: \(log(RT) \sim cue\_type + (1 + sub\_id \mid item)\).

Figure 2 shows the data distribution for each participant's RT (2B), the
estimates of condition means, and the full posterior distribution of
condition differences across the different cue types (2C). Children were
faster to identify the target animal while hearing its name
(\(M_{name}\) = 751.27 ms), as compared to its onomatopeic animal sound
(\(M_{onomatopeia}\) = 475.58 ms), and its vocalization
(\(M_{vocalization}\) = 636.62 ms). The difference between RTs for the
name and onomatopeic animal sounds was -161.04 ms with a HDI from
-353.64 ms to 12.30 ms. While the null value of zero difference falls
within the 95\% HDI, 96.70\% of the credible values fall below the null,
providing some evidence for faster processing of animal names. The
average difference in children's RT between the name and vocalization
trials was -275.69 ms with a HDI from -449.92 ms to -110.83 ms and
99.92\% of the credible values falling below zero, providing strong
evidence that children processed names more efficiently compared to
vocalizations. Finally, the average difference in children's RT between
the onomatopeic sounds and vocalization trials was -114.64 ms with a HDI
from -323.40 ms to 98.04 ms and 85.78\% of the credible values falling
below zero.

Together, the RT modeling results provide strong evidence that children
processed animal names around 276 ms faster than animal vocalizations,
with almost all of the estimates of the plausible RT differences falling
below the null value of zero. There was slightly weaker evidence that
children processed animal names more efficiently compared to onomatopeic
animal sounds but strong evidence of faster processing of onomatopeic
animal sounds compared to animal vocalizations. In sum, there was
evidence of a graded effect of cue type on RTs with names being faster
than onomatopeic animal sounds, which were faster than animal
vocalizations.

\begin{figure}[t]

{\centering \includegraphics[width=0.95\linewidth]{anime_manuscript_files/figure-latex/oc-plot-e1-1} 

}

\caption{Reaction Time (RT) results for Experiment 1. Panel A shows the timecourse of childrenâ€™s looking to the target animal after hearing a familiar animal name (black), onomatopoeic word (light grey), or animal vocalization (dark grey). Panel B shows the distribution of RT data across conditions. Each grey point shows the average RT for a single participant. The black box represent the most likely estimate of the condition means. Panel C shows the posterior distribution of plausible RT differences for each pair-wise comparison. Color represent the contrast. The black vertical dashed line represents the null value of zero condition difference. All error bars represent 95\% Highest Density Intervals.}\label{fig:oc-plot-e1}
\end{figure}

Next, we estimated children's attention to the target image over the
course of the trial to ask whether the three trial types were equally
effective cues to guide their attention to a familiar animal. The upper
left panel of Figure 3A shows children's proportion looking to the
target for each trial type. Visual inspection of the plot suggests two
things: (1) children reliably looked to the correct animal after hearing
each of the three familiar cues and (2) children's overall looking
behavior was strikingly similar across conditions.

Next, we quantified the strength of evidence for the absence of any
condition differences. We estimated the mean proportion looking for each
trial type using a Bayesian linear mixed-effects model with the same
specifications as the RT model described above. We transformed the
proportion looking scores using the empirical logit, with the final
model as: \(logit(accuracy) \sim cue\_type + (1 + sub\_id \mid item)\).
The black boxes in Figure 3A show mean Accuracy for each familiar cue
type (\(M_{name}\) = 0.66, \(M_{onomatopeia}\) = 0.68, and
(\(M_{vocalization}\) = 0.67). Children's looking to the target image
was reliably different from a model of random looking across all
conditions, with the null value of 0.5 falling well outside of the range
of plausible values (see the difference between the horizontal dashed
line and error bars in Figure 3A).

Moreover, the three cues types were equally effective in guiding
children's attention to the target animal over the course of the trial
as shown by the high overlap in the posterior distributions of the
accuracy measure with the null value of zero difference falling within
the 95\% HDI for all group comparisons (name vs.~onomatopeia:
\(\beta_{diff}\) = 0.03, 95\% HDI from -0.06 to 0.12; name
vs.~vocalization: \(\beta_{diff}\) = 0.01, 95\% HDI from -0.05 to 0.08;
onomatopeia vs.~vocalization: \(\beta_{diff}\) = -0.01, 95\% HDI from
-0.10 to 0.07). These results provide evidence that the animal
vocalizations were equally effective at directing looking behavior if
children have enough time to process the cue.

\hypertarget{using-novel-animal-names-and-animal-vocalization-to-disambiguate-novel-animals}{%
\subsubsection{Using novel animal names and animal vocalization to
disambiguate novel
animals}\label{using-novel-animal-names-and-animal-vocalization-to-disambiguate-novel-animals}}

Our next question is whether children would orient to a novel animal
after hearing a novel animal name or vocalization, thus showing evidence
of one-to-one biases for the vocalizations that animals produce. We
focus on children's accuracy, comparing their proportion of looking to
the novel animal against chance performance and across cue types. The
orange points in Figure 3A show Accuracy group means for each novel cue
type (\(M_{name}\) = 0.69, \(M_{vocalization}\) = 0.70). Children's
looking to the target image was reliably different from a model of
random looking behavior across both cue types, with the null value of
\(0.5\) falling well outside of the range of plausible values.

Moreover, the novel animal name and vocalizations were equally effective
in guiding children's attention to the target animal over the course of
the trial (name vs.~onomatopeia: \(\beta_{diff}\) = 0.01, 95\% HDI from
-0.05 to 0.08). This result provides converging evidence that the animal
vocalizations guided children's attentin in simlar ways if children have
enough time to process the cue.

\begin{figure}[t]

{\centering \includegraphics[width=0.85\linewidth]{anime_manuscript_files/figure-latex/acc-plot-e1-1} 

}

\caption{Accuracy results for Experiment 1 for familiar (upper row) and novel (lower row) trials. Panel A shows the data distribution and model estimates for Accuracy of children's looking behavior. Each point represents the proportion target looking for a single participant. Panel B shows the full posterior distribution plausbile condition differences in accuracy. The vertical dashed line represents the null value of zero condition difference. All other plotting conventions are the same as in Figure 2.}\label{fig:acc-plot-e1}
\end{figure}

Therefore, children appear to show one-to-one biases for the
vocalizations that animals produce already at 32 months of age, the
earliest age at which the disambiguation effect has been observed in a
domain other than word learning. There were no significant differences
between children's reaction times for novel animal names or
vocalizations.

\hypertarget{experiment-2}{%
\section{Experiment 2}\label{experiment-2}}

One issue that has received much attention in recent years concerns the
relation between children's referent selection and retention abilities.
While earlier studies tended to conflate disambiguation strategies and
children's word learning, more recent studies suggest that these two
abilities should not be conflated (Bion et al., 2013; Horst \&
Samuelson, 2008)

Horst and Samuelson (2008) examined both referent selection and
retention in four experiments with 2-year-olds. When children were shown
a novel object among familiar objects, they selected the novel object
when hearing a novel label, as found in previous studies. But
surprisingly, on retention trials 5 min later, these children showed no
evidence of remembering the names of the novel objects they had
previously identified. Using a looking-time task, Bion et al. (2013)
replicated these findings in a study with 18-, 24-, and 30-month-old
infants using looking time measures of performance.

Experiment 2 asks whether children can retain the link created through
disambiguation between a novel animal and a novel animal vocalization.
We also aim to replicate the findings from Experiment 1, showing that
children can identify familiar animals based on the vocalizations they
produce and use novel vocalizations to disambiguate novel animals. We
predict that children will succeed in disambiguation trials, but will
show little evidence of retention on subsequent disambiguation trials,
paralleling the findings of earlier studies with linguistic stimuli.

\hypertarget{method-1}{%
\subsection{Method}\label{method-1}}

\hypertarget{participants-1}{%
\subsubsection{Participants}\label{participants-1}}

Participants were 23 31-month-old children (M = 31.10 months; range = 30
- 32), 12 girls. All children were typically developing and from
families where English was the dominant language.

\hypertarget{visual-stimuli-1}{%
\subsubsection{Visual stimuli}\label{visual-stimuli-1}}

The visual stimuli were the same as in Experiment 1, except for the
novel animals (aardvark and capybara), which replaced the novel animals
(pangolin and tapir) used in Experiment 1 (see animals in Figure 4). We
decided to change the novel animals to test whether our results would
generalize beyond the particular stimulus set in Experiment 1. All
children were reported by parents to have had no exposure to the novel
animals.

\hypertarget{auditory-stimuli-1}{%
\subsubsection{Auditory stimuli}\label{auditory-stimuli-1}}

The auditory stimuli consisted of the same familiar and novel animal
vocalizations as in Experiment 1.

\begin{figure}[t]

{\centering \includegraphics[width=0.7\linewidth]{anime_manuscript_files/figure-latex/stimuli-e2-1} 

}

\caption{Trial types in Experiments 4 organized by type of trial. Children hear familiar and novel vocalizations. The target animal for each trial type is on the left.}\label{fig:stimuli-e2}
\end{figure}

\hypertarget{familiarization-books-1}{%
\subsubsection{Familiarization books}\label{familiarization-books-1}}

As in Experiment 1, we sent home a children's book to ensure that all
participants had at least some exposure to the familiar animals and
auditory cues. Since we were interested in the natural animal
vocalizations and not the names/lexical sounds, we only sent the Hear
and There Sounds on the Farm book. Instructions were the same as
Experiment 1, and the book was sent a week before the visit.

\hypertarget{procedure-1}{%
\subsubsection{Procedure}\label{procedure-1}}

Experiment 2 consisted of one visit. Each child saw 30 trials,
consisting of three trial types (Figure 4). The 16 familiar trials and
eight disambiguation trials were identical to the vocalization trials in
Experiment 1. On six retention trials, the two novel animals were
presented side by side, with each serving as the target three times. The
same coding and speed/accuracy measures were used as in Experiment 1.

\hypertarget{results-and-discussion-1}{%
\subsection{Results and discussion}\label{results-and-discussion-1}}

\hypertarget{retention-of-the-link-between-a-novel-animal-and-a-novel-vocalization}{%
\subsubsection{Retention of the link between a novel animal and a novel
vocalization:}\label{retention-of-the-link-between-a-novel-animal-and-a-novel-vocalization}}

Figure 5A shows children's proportion looking to the target animal after
hearing a familiar or a novel animal vocalization over the same analysis
window used in Experiment 1 (300 to 2500 ms). Visual inspection of the
figure suggests that children successfully oriented to the target image
after hearing both familiar and novel animal vocalizations. The black
boxes show mean proportion target looking for familiar (\(M_{familiar}\)
= 0.66, disambiguation (\(M_{disambiguation}\) = 0.64, and retention
trials (\(M_{retention}\) = 0.50).

Children's looking to the target image was reliably different from
random-looking behavior for both familiar and disambiguation trials,
with the null value of 0.50 falling well outside of the range of
plausible values. Moreover, the novel animal vocalizations and the
familiar animal vocalizations were equally effective in guiding
children's attention to the target animal over the course of the trial
(familiar vs.~disambiguation: \(\beta_{diff}\) = 0.01, 95\% HDI from
-0.08 to 0.11).

In contrast to children's success on familiar and disambiguation trials,
they did not show evidence of retaining the link between the novel
animal and the novel animal vocalization, with the null value of \(0.5\)
proportion looking falling well within the range of plausible estimates.
Moreover, there was strong evidence that children were less accurate on
retention trials compared to both disambiguation trials
(\(\beta_{diff}\) = 0.14, 95\% HDI from 0.03 to 0.26) and familiar
trials (\(\beta_{diff}\) = 0.16, 95\% HDI from 0.05 to 0.27).

\begin{figure}[t]

{\centering \includegraphics[width=0.85\linewidth]{anime_manuscript_files/figure-latex/acc-plot-e2-1} 

}

\caption{Proportion target looking for  familiar and novel animal vocalizations in Experiment 2. Panel A shows the data distribution and the model estimates of mean Accuracy for the different trial types. Panel B shows the full posterior distribution of plausbile condition differences for all condition contrasts. The vertical dashed line represents the null value of zero condition difference. All other plotting conventions are the same as in Figures 2 and 3.}\label{fig:acc-plot-e2}
\end{figure}

Three findings emerged from the accuracy analysis: First, children
oriented to a familiar animal after hearing a familiar animal
vocalization. Second, children oriented to a novel animal after hearing
a novel animal vocalization. These two results are an internal
replication of the key findings from Experiment 1 in a new sample. Also,
we found that children did not show evidence of retaining the link
between a novel animal vocalization and a novel animal.

\hypertarget{general-discussion}{%
\section{General Discussion}\label{general-discussion}}

Three main findings emerged from this work. First, 32-month-olds
responded fastest to a familiar animal name and slowest to a familiar
animal vocalization, with onomatopoeic sounds falling in between.
Children could, however, identify the familiar animals after hearing any
of on these three sound types. The second finding was that children
showed disambiguation biases for the types of vocalizations that animals
produce, similar to their biases in word learning and communicative
contexts. The third finding was that these biases do not necessarily
lead to learning, as children were not able to retain the link between
novel animals and their vocalizations. This lack of retention parallels
the findings of recent word-learning studies (Bion et al., 2013; Horst
\& Samuelson, 2008; McMurray, Horst, Toscano, \& Samuelson, 2009) and
emphasizes the theoretical importance of disentangling processes of
disambiguating reference, an in-the-moment phenomenon, and word
learning, which occurs over a longer timescale.

In our study, we found a processing speed advantage for words over other
meaningful sounds. Some theories of language development argue that
words are unique stimuli because they refer to objects in the world
(Waxman \& Gelman, 2009), while other theories argue that words are
special because they activate conceptual information more quickly,
accurately, and in a more categorical way than nonverbal sounds
(Edmiston \& Lupyan, 2015). It is also possible that words and nonverbal
sounds might be processed by different brain regions, with words being
accessed more rapidly. A second explanation for the advantage for words
might be differences in sheer frequency in the input. At least in our
sample, it is safe to assume that children have heard the word cow many
more times than they have heard an actual cow mooing. Frequency effects
have been robustly demonstrated in the processing of words, with adults
being faster to recognize words that they hear more frequently (Dahan,
Magnuson, \& Tanenhaus, 2001). A final explanation is that words are
very effective at presenting a lot of information in a short period.
When children see a simple visual world consisting of a dog and a sheep,
the first phoneme of the target word is already sufficient to determine
the animal that is likely to be talked about next.

Much less is known about children's and adults' processing of
onomatopoeic sounds. Hashimoto et al. (2006) compared brain responses to
nouns, animal sounds, and onomatopoetic sounds, and found that
onomatopoeic sounds were processed by extensive brain regions involved
in the processing of both verbal and nonverbal sounds. Cummings et al.
(2009) argues that onomatopoeic sounds might provide young children with
information about intermodal associations, bridging their understanding
of non-arbitrary environmental sounds and arbitrary word-object
associations. Fernald and Morikawa (1993) reported that 52\% of Japanese
mothers used onomatopoeic sounds to label target objects, while only 1
in 30 American mothers did so. While our results do not speak directly
to these theoretical issues, they do suggest that onomatopoeic sounds
function like words in that they are capable of activating conceptual
representations that drive children's visual attention to seek the
physical referent of the sound. However, there was some evidence that
children processed onomatopeic sounds less efficiently compared to words
in our task.

Our second finding was that children looked at a novel animal when
hearing a novel animal vocalization, with accuracy comparable to their
disambiguation of novel animal names. Bloom (2002) outlines three
different theories that could explain children's disambiguation biases.
These biases could be a specifically lexical phenomenon that applies
only to words (lexical account), a product of children's theory of mind
restricted to communicative situations (pragmatic account), or a special
case of a general principle of learning that exaggerates regularities
across domains (domain-general account). By using animal sounds, this
study provides an important empirical result since the theoretical
accounts make different predictions for children's looking behavior in
response to a stimulus that is non-linguistic and non-communicative.

Previous studies contrasted lexical-specific and pragmatic accounts. For
example, Diesendruck and Markson (2001) found that children expect
speakers to use consistent facts to refer to objects, and they select a
novel object when hearing a novel fact. Recent studies suggest that
different strategies might be used to make inferences about speakers'
communicative intent and the meaning of a novel word. Autistic children
who are known to have pragmatic deficits show disambiguation biases and
select a novel word when hearing a novel object (Preissler \& Carey,
2005). Moreover, disambiguation biases for words are correlated with
vocabulary, and disambiguation biases for facts are correlated with
social-pragmatic skills (Marchena et al., 2011). These findings suggest
that disambiguation biases for words might not be motivated uniquely by
pragmatic inferences, but they do not provide evidence for or against
domain-general accounts of the disambiguation bias.

Relatively few studies have looked at disambiguation biases in
non-linguistic domains. Moher, Feigenson, and Halberda (2010) showed
that three-year-olds link different voices to unique faces, showing that
one-to-one biases might extend to other communicative domains. However,
Yoshida, Rhemtulla, and Vouloumanos (2012) found that adults in a
statistical learning task were less likely to show evidence of using
disambiguation biases to learn nonspeech sounds even though this
behavior would have facilitated task performance. These results suggest
that at some point in development disambiguation constraints may operate
more strongly over speech compared to nonspeech sounds. Critically,
these results do not provide evidence against a domain-general account
since participants had no reason to expect that the mapping between
random non-linguistic sounds and objects should be mutually exclusive.
It could be that similar learning strategies might be applied to
non-linguistic sounds when they become meaningfully related to objects
in the environment or relevant for communication with other people.

The work reported here demonstrates that young children do show evidence
of disambiguation biases in a non-linguistic and non-communicative
domain. These findings are consistent with predictions made by
domain-general accounts that explain disambiguation biases as the
byproduct of a system that attempts to find regularities in complex
learning tasks that involve consistent mappings. Previous Connectionist
and Bayesian models of word learning showed that disambiguation biases
emerge without built-in constraints on the types of meanings words could
have (Frank et al., 2009; McMurray et al., 2012; Regier, 2003). In
principle, these same biases could emerge if these models attempted to
map animal vocalizations to animals and there were consistent
co-occurrences present in the learning environment.

One open question is whether a single disambiguation mechanism can
account for the diverse set of contexts in which children show
disambiguation behaviors. Here, we found that children could
disambiguate stimuli other than words and facts, suggesting at least the
existence of a domain-general mechanism that leads to disambiguation. A
preference for parsimony might favor a single mechanism. But as pointed
out by recent computational work, it is possible that different
mechanisms jointly contribute to disambiguation behavior, explaining the
diverse set of contexts and populations in which disambiguation behavior
has been observed (Lewis \& Frank, 2013). Thus, it is possible that the
same behavior - selecting a novel object when hearing a novel auditory
stimulus - might result from different computational mechanisms
depending on the task at hand, children's age, or the particular stimuli
set and assumptions about the people involved in the interaction. Put
another way, children could use a domain-general mechanism to learn
about novel animal vocalizations, and they could use a lexical and
pragmatic constraint to learn about novel words.

Our third finding was that one-to-one biases for animal vocalizations do
not necessarily lead to retention of the link between a novel animal and
a novel vocalization. This finding dovetails with recent
cross-situational models of early word learning that emphasize the
separate processes of disambiguating reference and learning word-object
labels over time (McMurray et al., 2012). McMurray and colleagues
propose that referent-selection requires that children give their best
guess about a new word's meaning in a specific ambiguous situation, but
that learning operates over a developmental timescale, requiring
multiple exposures to build up stable word-object links. Although
disambiguation can be viewed as the product of learning that has
occurred up to that point, for younger children it does not necessarily
result in learning. These claims are also supported by evidence from
studies on early word learning using online and offline measures of
retention (Bion et al., 2013).

These results also add to a recent body of work that encourages us to
think differently about disambiguation biases. This work has emphasized
the role of experience, showing that the tendency to select a novel
object when hearing a novel word is not robustly present across
populations. For example, bilingual children, children from lower
socioeconomic status, children who receive less language input, and
children with less structured vocabularies or smaller vocabulary sizes,
take longer to show evidence of disambiguation biases (Bion et al.,
2013; Yurovsky, Bion, Smith, \& Fernald, 2012). Moreover, the current
study adds to recent studies taking a fresh look at an old question: the
scope of disambiguation biases (Suanda \& Namy, 2012, 2013). Taken
together, these findings suggest that it is useful to consider
variability in the emergence, function, and scope of language learning
processes that might be characterized as universal based on studies
using a particular population or a specific stimulus type.

Finally, our results emphasize that children's learning about objects in
their environment involves more than learning their names. Before object
names are learned, sounds and actions might form the basis on which
objects are conceptualized. For example, children might see barking as a
defining feature of dogs and may say bow-wow in response to the picture
of a dog, even before they learn the animal name (Nelson, 1974).
Learning the meaning of an object, therefore, requires learning several
cross-modal associations, including learning the object's texture,
smell, as well as its sounds and names. Children do not have explicit
constraints that freshly baked cookies should have only one smell. Yet,
they might recognize and get excited about the familiar smell coming
from the kitchen and might assume their mothers are baking something new
when smelling something unfamiliar.

\hypertarget{conclusions}{%
\subsection{Conclusions}\label{conclusions}}

Children use different types of knowledge to make sense of a constantly
changing world. They might identify animal vocalizations based on the
shape of the vocal tract of the animal, its location and size, and their
previous knowledge about animal vocalizations. Importantly, these cues
normally converge in helping children identify an animal in the
environment. The same is true for their identification of referents for
words. Children can identify the referent for a word based on semantics
(Goodman, McDonough, \& Brown, 1998), cross-situational statistics
(Smith \& Yu, 2008), syntax (Brown, 1957), pragmatic and social cues
(Baldwin, 1993), and disambiguation biases (Markman, 1991). As children
grow older, these different sources of information provide converging
evidence that a novel word should refer to a novel object. Children can
rely on their knowledge about the world, speakers, and on their previous
experiences with words to figure out what speakers are talking about --
a task we continue to do throughout our lives when learning new words
and interpreting complex sentences.

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-R-papaja}{}%
Aust, F., \& Barth, M. (2017). \emph{papaja: Create APA manuscripts with
R Markdown}. Retrieved from \url{https://github.com/crsh/papaja}

\leavevmode\hypertarget{ref-baldwin1993infants}{}%
Baldwin, D. A. (1993). Infants' ability to consult the speaker for clues
to word reference. \emph{Journal of Child Language}, \emph{20}(02),
395--418.

\leavevmode\hypertarget{ref-bates1989functionalism}{}%
Bates, E., MacWhinney, B., \& others. (1989). Functionalism and the
competition model. \emph{The Crosslinguistic Study of Sentence
Processing}, \emph{3}, 73--112.

\leavevmode\hypertarget{ref-bion2013fast}{}%
Bion, R. A., Borovsky, A., \& Fernald, A. (2013). Fast mapping, slow
learning: Disambiguation of novel word--object mappings in relation to
vocabulary learning at 18, 24, and 30months. \emph{Cognition},
\emph{126}(1), 39--53.

\leavevmode\hypertarget{ref-bloom2002children}{}%
Bloom, P. (2002). \emph{How children learn the meaning of words}. The
MIT Press.

\leavevmode\hypertarget{ref-brown1957linguistic}{}%
Brown, R. W. (1957). Linguistic determinism and the part of speech.
\emph{The Journal of Abnormal and Social Psychology}, \emph{55}(1), 1.

\leavevmode\hypertarget{ref-chen2011crossmodal}{}%
Chen, Y.-C., \& Spence, C. (2011). Crossmodal semantic priming by
naturalistic sounds and spoken words enhances visual sensitivity.
\emph{Journal of Experimental Psychology: Human Perception and
Performance}, \emph{37}(5), 1554.

\leavevmode\hypertarget{ref-chen2013time}{}%
Chen, Y.-C., \& Spence, C. (2013). The time-course of the cross-modal
semantic modulation of visual picture processing by naturalistic sounds
and spoken words. \emph{Multisensory Research}, \emph{26}(4), 371--386.

\leavevmode\hypertarget{ref-clark1990pragmatics}{}%
Clark, E. V. (1990). On the pragmatics of contrast. \emph{Journal of
Child Language}, \emph{17}(2), 417--431.

\leavevmode\hypertarget{ref-cummings2010verbal}{}%
Cummings, A., \& Ceponiene, R. (2010). Verbal and nonverbal semantic
processing in children with developmental language impairment.
\emph{Neuropsychologia}, \emph{48}(1), 77--85.

\leavevmode\hypertarget{ref-cummings2006auditory}{}%
Cummings, A., Ceponiene, R., Koyama, A., Saygin, A., Townsend, J., \&
Dick, F. (2006). Auditory semantic networks for words and natural
sounds. \emph{Brain Research}, \emph{1115}(1), 92--107.

\leavevmode\hypertarget{ref-cummings2009infants}{}%
Cummings, A., Saygin, A. P., Bates, E., \& Dick, F. (2009). Infants'
recognition of meaningful verbal and nonverbal sounds. \emph{Language
Learning and Development}, \emph{5}(3), 172--190.

\leavevmode\hypertarget{ref-dahan2001time}{}%
Dahan, D., Magnuson, J. S., \& Tanenhaus, M. K. (2001). Time course of
frequency effects in spoken-word recognition: Evidence from eye
movements. \emph{Cognitive Psychology}, \emph{42}(4), 317--367.

\leavevmode\hypertarget{ref-diesendruck2001children}{}%
Diesendruck, G., \& Markson, L. (2001). Children's avoidance of lexical
overlap: A pragmatic account. \emph{Developmental Psychology},
\emph{37}(5), 630--641.

\leavevmode\hypertarget{ref-edmiston2015makes}{}%
Edmiston, P., \& Lupyan, G. (2015). What makes words special? Words as
unmotivated cues. \emph{Cognition}, \emph{143}, 93--100.

\leavevmode\hypertarget{ref-fernald1993common}{}%
Fernald, A., \& Morikawa, H. (1993). Common themes and cultural
variations in japanese and american mothers' speech to infants.
\emph{Child Development}, \emph{64}(3), 637--656.

\leavevmode\hypertarget{ref-fernald1998rapid}{}%
Fernald, A., Pinto, J. P., Swingley, D., Weinbergy, A., \& McRoberts, G.
W. (1998). Rapid gains in speed of verbal processing by infants in the
2nd year. \emph{Psychological Science}, \emph{9}(3), 228--231.

\leavevmode\hypertarget{ref-fernald2008looking}{}%
Fernald, A., Zangl, R., Portillo, A. L., \& Marchman, V. A. (2008).
Looking while listening: Using eye movements to monitor spoken language.
\emph{Developmental Psycholinguistics: On-Line Methods in Children's
Language Processing}, \emph{44}, 97.

\leavevmode\hypertarget{ref-frank2009using}{}%
Frank, M. C., Goodman, N. D., \& Tenenbaum, J. B. (2009). Using
speakers' referential intentions to model early cross-situational word
learning. \emph{Psychological Science}, \emph{20}(5), 578--585.

\leavevmode\hypertarget{ref-fulkerson2007words}{}%
Fulkerson, A. L., \& Waxman, S. R. (2007). Words (but not tones)
facilitate object categorization: Evidence from 6-and 12-month-olds.
\emph{Cognition}, \emph{105}(1), 218--228.

\leavevmode\hypertarget{ref-gabry2016rstanarm}{}%
Gabry, J., \& Goodrich, B. (2016). Rstanarm: Bayesian applied regression
modeling via stan. R package version 2.10. 0.

\leavevmode\hypertarget{ref-goodman1998role}{}%
Goodman, J. C., McDonough, L., \& Brown, N. B. (1998). The role of
semantic context and memory in the acquisition of novel nouns.
\emph{Child Development}, \emph{69}(5), 1330--1344.

\leavevmode\hypertarget{ref-hashimoto2006neural}{}%
Hashimoto, T., Usui, N., Taira, M., Nose, I., Haji, T., \& Kojima, S.
(2006). The neural mechanism associated with the processing of
onomatopoeic sounds. \emph{Neuroimage}, \emph{31}(4), 1762--1770.

\leavevmode\hypertarget{ref-horst2008fast}{}%
Horst, J. S., \& Samuelson, L. K. (2008). Fast mapping but poor
retention by 24-month-old infants. \emph{Infancy}, \emph{13}(2),
128--157.

\leavevmode\hypertarget{ref-lewis2013modeling}{}%
Lewis, M., \& Frank, M. (2013). Modeling disambiguation in word learning
via multiple probabilistic constraints. In \emph{Proceedings of the
annual meeting of the cognitive science society} (Vol. 35).

\leavevmode\hypertarget{ref-lupyan2012evocative}{}%
Lupyan, G., \& Thompson-Schill, S. L. (2012). The evocative power of
words: Activation of concepts by verbal and nonverbal means.
\emph{Journal of Experimental Psychology: General}, \emph{141}(1), 170.

\leavevmode\hypertarget{ref-de2011mutual}{}%
Marchena, A. de, Eigsti, I.-M., Worek, A., Ono, K. E., \& Snedeker, J.
(2011). Mutual exclusivity in autism spectrum disorders: Testing the
pragmatic hypothesis. \emph{Cognition}, \emph{119}(1), 96--113.

\leavevmode\hypertarget{ref-markman1991whole}{}%
Markman, E. M. (1991). The whole-object, taxonomic, and mutual
exclusivity assumptions as initial constraints on word meanings.
\emph{Perspectives on Language and Thought: Interrelations in
Development}, 72--106.

\leavevmode\hypertarget{ref-mccleery2010neural}{}%
McCleery, J. P., Ceponiene, R., Burner, K. M., Townsend, J., Kinnear,
M., \& Schreibman, L. (2010). Neural correlates of verbal and nonverbal
semantic integration in children with autism spectrum disorders.
\emph{Journal of Child Psychology and Psychiatry}, \emph{51}(3),
277--286.

\leavevmode\hypertarget{ref-mcmurray2012word}{}%
McMurray, B., Horst, J. S., \& Samuelson, L. K. (2012). Word learning
emerges from the interaction of online referent selection and slow
associative learning. \emph{Psychological Review}, \emph{119}(4), 831.

\leavevmode\hypertarget{ref-mcmurray2009integrating}{}%
McMurray, B., Horst, J. S., Toscano, J. C., \& Samuelson, L. K. (2009).
Integrating connectionist learning and dynamical systems processing:
Case studies in speech and lexical development. \emph{Toward a Unified
Theory of Development: Connectionism and Dynamic Systems Theory
Re-Considered}, 218--249.

\leavevmode\hypertarget{ref-moher2010one}{}%
Moher, M., Feigenson, L., \& Halberda, J. (2010). A one-to-one bias and
fast mapping support preschoolers' learning about faces and voices.
\emph{Cognitive Science}, \emph{34}(5), 719--751.

\leavevmode\hypertarget{ref-murray2006rapid}{}%
Murray, M. M., Camen, C., Andino, S. L. G., Bovet, P., \& Clarke, S.
(2006). Rapid brain discrimination of sounds of objects. \emph{Journal
of Neuroscience}, \emph{26}(4), 1293--1302.

\leavevmode\hypertarget{ref-R-here}{}%
MÃ¼ller, K. (2017). \emph{Here: A simpler way to find your files}.
Retrieved from \url{https://CRAN.R-project.org/package=here}

\leavevmode\hypertarget{ref-namy1998words}{}%
Namy, L. L., \& Waxman, S. R. (1998). Words and gestures: Infants'
interpretations of different forms of symbolic reference. \emph{Child
Development}, \emph{69}(2), 295--308.

\leavevmode\hypertarget{ref-nelson1974concept}{}%
Nelson, K. (1974). Concept, word, and sentence: Interrelations in
acquisition and development. \emph{Psychological Review}, \emph{81}(4),
267.

\leavevmode\hypertarget{ref-pena2003sounds}{}%
Pena, M., Maki, A., Kovac, D., Dehaene-Lambertz, G., Koizumi, H.,
Bouquet, F., \& Mehler, J. (2003). Sounds and silence: An optical
topography study of language recognition at birth. \emph{Proceedings of
the National Academy of Sciences}, \emph{100}(20), 11702--11705.

\leavevmode\hypertarget{ref-preissler2005role}{}%
Preissler, M. A., \& Carey, S. (2005). The role of inferences about
referential intent in word learning: Evidence from autism.
\emph{Cognition}, \emph{97}(1), B13--B23.

\leavevmode\hypertarget{ref-quine19600}{}%
Quine, W. V. (1960). 0. Word and object. \emph{111e MIT Press}.

\leavevmode\hypertarget{ref-R-base}{}%
R Core Team. (2017). \emph{R: A language and environment for statistical
computing}. Vienna, Austria: R Foundation for Statistical Computing.
Retrieved from \url{https://www.R-project.org/}

\leavevmode\hypertarget{ref-regier2003emergent}{}%
Regier, T. (2003). Emergent constraints on word-learning: A
computational perspective. \emph{Trends in Cognitive Sciences},
\emph{7}(6), 263--268.

\leavevmode\hypertarget{ref-scofield2007two}{}%
Scofield, J., \& Behrend, D. A. (2007). Two-year-olds differentially
disambiguate novel words and facts. \emph{Journal of Child Language},
\emph{34}(4), 875--889.

\leavevmode\hypertarget{ref-smith2008infants}{}%
Smith, L. B., \& Yu, C. (2008). Infants rapidly learn word-referent
mappings via cross-situational statistics. \emph{Cognition},
\emph{106}(3), 1558--1568.

\leavevmode\hypertarget{ref-suanda2012detailed}{}%
Suanda, S. H., \& Namy, L. L. (2012). Detailed behavioral analysis as a
window into cross-situational word learning. \emph{Cognitive Science},
\emph{36}(3), 545--559.

\leavevmode\hypertarget{ref-suanda2013young}{}%
Suanda, S. H., \& Namy, L. L. (2013). Young word learners'
interpretations of words and symbolic gestures within the context of
ambiguous reference. \emph{Child Development}, \emph{84}(1), 143--153.

\leavevmode\hypertarget{ref-uddin2018understanding}{}%
Uddin, S., Heald, S. L., Van Hedger, S. C., Klos, S., \& Nusbaum, H. C.
(2018a). Understanding environmental sounds in sentence context.
\emph{Cognition}, \emph{172}, 134--143.

\leavevmode\hypertarget{ref-uddin2018hearing}{}%
Uddin, S., Heald, S. L., Van Hedger, S. C., \& Nusbaum, H. C. (2018b).
Hearing sounds as words: Neural responses to environmental sounds in the
context of fluent speech. \emph{Brain and Language}, \emph{179}, 51--61.

\leavevmode\hypertarget{ref-vouloumanos2009five}{}%
Vouloumanos, A., Druhen, M. J., Hauser, M. D., \& Huizink, A. T. (2009).
Five-month-old infants' identification of the sources of vocalizations.
\emph{Proceedings of the National Academy of Sciences}, \emph{106}(44),
18867--18872.

\leavevmode\hypertarget{ref-vouloumanos2004tuned}{}%
Vouloumanos, A., \& Werker, J. F. (2004). Tuned to the signal: The
privileged status of speech for young infants. \emph{Developmental
Science}, \emph{7}(3), 270--276.

\leavevmode\hypertarget{ref-vouloumanos2007listening}{}%
Vouloumanos, A., \& Werker, J. F. (2007a). Listening to language at
birth: Evidence for a bias for speech in neonates. \emph{Developmental
Science}, \emph{10}(2), 159--164.

\leavevmode\hypertarget{ref-vouloumanos2007voice}{}%
Vouloumanos, A., \& Werker, J. F. (2007b). Why voice melody alone cannot
explain neonates' preference for speech. \emph{Developmental Science},
\emph{10}(2), 169.

\leavevmode\hypertarget{ref-waxman2009early}{}%
Waxman, S. R., \& Gelman, S. A. (2009). Early word-learning entails
reference, not merely associations. \emph{Trends in Cognitive Sciences},
\emph{13}(6), 258--263.

\leavevmode\hypertarget{ref-R-tidyverse}{}%
Wickham, H. (2017). \emph{Tidyverse: Easily install and load the
'tidyverse'}. Retrieved from
\url{https://CRAN.R-project.org/package=tidyverse}

\leavevmode\hypertarget{ref-woodward1999infants}{}%
Woodward, A., \& Hoyne, K. (1999). Infants' learning about words and
sounds in relation to objects. \emph{Child Development}, \emph{70}(1),
65--77.

\leavevmode\hypertarget{ref-R-knitr}{}%
Xie, Y. (2015). \emph{Dynamic documents with R and knitr} (2nd ed.).
Boca Raton, Florida: Chapman; Hall/CRC. Retrieved from
\url{https://yihui.name/knitr/}

\leavevmode\hypertarget{ref-xu2002role}{}%
Xu, F. (2002). The role of language in acquiring object kind concepts in
infancy. \emph{Cognition}, \emph{85}(3), 223--250.

\leavevmode\hypertarget{ref-yoshida2012exclusion}{}%
Yoshida, K., Rhemtulla, M., \& Vouloumanos, A. (2012). Exclusion
constraints facilitate statistical word learning. \emph{Cognitive
Science}, \emph{36}(5), 933--947.

\leavevmode\hypertarget{ref-yurovsky2012mutual}{}%
Yurovsky, D., Bion, R. A., Smith, L. B., \& Fernald, A. (2012). Mutual
exclusivity and vocabulary structure. \emph{N. Miyake, D. Peebles, \& RP
Cooper (Eds.)}, 1197--1202.






\end{document}
