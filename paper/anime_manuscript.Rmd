---
title             : "Moos as cues: Two-year-olds expect one-to-one mappings in a non-linguistic and non-communicative domain"
shorttitle        : "Moos as cues"

author: 
  - name          : "Kyle MacDonald"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "2225 Rolfe Hall, Los Angeles, CA 90095"
    email         : "kemacdonald@ucla.edu"
  - name          : "Ricardo A. H. Bion"
    affiliation   : "2"
  - name          : "Anne Fernald"
    affiliation   : "2"

affiliation:
  - id            : "1"
    institution   : "University of California, Los Angeles"
  - id            : "2"
    institution   : "Stanford University"

author_note: |
  

abstract: |
  When hearing a novel name, children tend to select a novel object rather than a familiar one: A bias that can facilitate word learning in ambiguous contexts. But what cognitive processes explain this behavior? Different theoretical accounts have proposed explanations based on constraints on the structure of the lexicon, pragmatic inferences about speakers' communicative intent, and domain-general inferences. Here, we ask whether disambiguation behaviors emerge in a domain that is non-linguistic and non-communicative but in which there are strong regularities: animal vocalizations. Using real-time processing measures, we show that two-year-olds identify familiar animals based on their vocalizations, though not as fast as when hearing their names or their onomatopoeic labels. We then show that children tend to look at an unfamiliar animal when hearing a novel animal vocalization or novel animal name, responding with similar efficiency to both cues. In Experiment 2, we replicate the key finding that children can disambiguate novel animal vocalizations but find that disambiguation does not necessarily lead to retention measured at a longer timescale. These results are consistent with an account of disambiguation arising from domain-general inferences, rather than from lexical or communicative constraints.
  
keywords          : "disambiguation, mutual exclusivity, environmental sounds, retention, word learning"
wordcount         : "7946"

bibliography      : ["anime.bib", "r-references.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "english"
class             : "man"
output            : 
  #- papaja::apa6_pdf
  - papaja::apa6_word
---

```{r load-packages, include = FALSE}
library("papaja"); library(knitr); library(png); library(here); library(tidyverse)
```

```{r global-options, include=FALSE}
knitr::opts_chunk$set(cache = T, fig.width = 5, fig.asp = 0.618, fig.align = "center")
```

```{r load-data-models}
d <- read_csv(here("data/03_summary_tables", "anime_trial_level.csv"))
d_models <- read_rds(here("data/03_summary_tables", "anime-posterior-samples.rds"))
```

```{r r-refs}
r_refs(file = "r-references.bib")
my_citations <- cite_r(file = "r-references.bib", 
                       pkgs = c("tidyverse", "rstanarm", "papaja", "here", "knitr"), 
                       withhold = FALSE,
                       footnote = TRUE)
```

# Introduction

When children encounter people and animals in daily life, they experience them through multiple sensory modalities simultaneously. When playing with pets, for example, a child could learn to link the physical features and actions of a dog with its barking sound or with the onomatopoeic word resembling its characteristic vocalization, such as woof-woof. Eventually, the child will learn to associate the word "dog" with these other features. That is, just as familiar object labels are consistently associated with particular referents, animal vocalizations and onomatopoeic words for vocalizations also provide consistent associations between an auditory stimulus and an object in the visual world. Unlike words, however, animal vocalizations are not a linguistic or communicative stimulus. Together, these features make animal vocalizations a particularly interesting stimulus for testing theories of children's disambiguation behavior and asking whether children respond to words as a unique kind of auditory stimulus. Here, we compare young children’s efficiency in using these different sounds as cues to identifying particular animals. In addition, we investigate whether children can use disambiguation strategies, which have frequently been characterized as pragmatic or lexically-specific in nature (Bloom, 2002; Diesendruck & Markson, 2001; Markman, 1991) to infer which of two animals is associated with a novel animal vocalization.

The question of whether words are a special kind of stimulus has a rich tradition in the cognitive sciences. Several studies have found advantages for speech sounds over tones in object individuation and categorization in young infants [@fulkerson2007words; @xu2002role]. Focusing on associations between objects and sounds, objects and tones, or objects and gestures, several studies found that younger infants accept several different forms as potential object labels, but that older infants are more discriminating and show a preference for words [@namy1998words; @woodward1999infants]. Another line of research found that infants prefer to hear spoken words over some non-linguistic analogs [@vouloumanos2004tuned; @vouloumanos2007listening; @vouloumanos2007voice] and that the neonate brain responds differently to speech as compared to backward speech [@pena2003sounds]. These studies found advantages for speech over non-linguistic analogs in categorization, individuation, crossmodal association, and speech preferences. However, they all used arbitrary, non-linguistic cues (e.g., tones) that are not consistently associated with objects in children's everyday environments.
 
Other research has approached the question of whether speech is special from a different perspective, comparing how people process spoken words as compared to non-arbitrary environmental sounds, such as animal vocalizations (e.g., cat meowing) or the sounds produced by inanimate objects (e.g., car starting). Studies with adults have found similarities and differences in both behavioral and neural responses to cross-modal semantic associations between words and environmental sounds. In a picture detection task, @chen2011crossmodal found a facilitation effect for environmental sounds but not for words when the onset of the auditory stimulus preceded the image by approximately 350 ms (see also @chen2013time). They suggest that recognition of environmental sounds is faster because words must also be processed at a lexical stage, while environmental sounds activate semantic representations directly. In contrast, @lupyan2012evocative found a processing advantage for words as compared to environmental sounds (see also @edmiston2015makes). This finding was interpreted as evidence that words evoke category representations that are decoupled from surface-level features of a particular category member, leading to faster processing.

In an ERP study with adults, @cummings2006auditory found that largely overlapping neural networks processed verbal and non-verbal meaningful sounds. In another study focusing on three different sound types that varied in arbitrariness, @hashimoto2006neural found different neural mechanisms for the processing of animal names and vocalizations, with onomatopoeic words activating both areas. Recent work by @uddin2018understanding showed that adults were able to use prior sentential context to facilitate recognition of both speech and environmental sounds, also finding evidence of parallel neural responses using EEG measures [@uddin2018hearing] (see also @van1995conceptual). Other work using EEG has found evidence that words are stored in semantic memory with a more "fine-grained or graded" structure compared to nonlinguistic auditory signals [@hendrickson2015organization].

Because research on environmental sounds is relatively new, it is challenging to reconcile these discrepant findings. Variations in tasks or timing of stimuli could influence results, and different theoretical commitments can lead to different interpretations. For example, environmental sounds are often treated as encompassing both the sounds of living and human-made objects (e.g., cow mooing, bell ringing) despite evidence that these sounds are interpreted differently by the adult brain [@murray2006rapid; lewis2004human]. Nevertheless, this line of research provides promising new ways to examine the question of whether language emerges from the interaction of domain-general cognitive processes or domain-specific mechanism [@bates1989functionalism]. For example, recent research comparing the processing of speech and non-speech sounds is leading to new insights relevant to autism, developmental language impairment, and cochlear implants [@cummings2010verbal; @mccleery2010neural].

From a developmental perspective, it is also important to understand whether children's processing of words and non-arbitrary, non-linguistic sounds changes as a function of experience with these cues. However, few studies have examined this question. Using a preferential-looking paradigm, @cummings2009infants found that 15- and 25-mo-olds could use words and environmental sounds to guide their attention to familiar objects, becoming more efficient with both cues as they get older. @vouloumanos2009five found that 5-month-olds can match some animals to the vocalizations they produce. And studies with children with autism and developmental language impairment found more severe deficits for the processing of words than environmental sounds [@cummings2010verbal; @mccleery2010neural]. And work with older children has shown that environmental sounds can be used as conceptual primes, facilitating 7-8 year-olds' performance on a recognition memory task [@geurten2017hearing].

We build on this developmental approach, using the looking-while-listening paradigm [@fernald2008looking] to compare children's real-time processing of different auditory stimuli that are consistently associated with familiar animals but vary in arbitrariness. First, we ask whether 32-month-olds can use animal vocalizations (e.g., dog barking), onomatopoeic sounds (e.g., bow-wow), and familiar animal names (e.g., dog) to identify familiar animals. By using real-time processing measures, we can measure the timecourse of recognition and ask whether these three sounds are equally effective as acoustic cues in guiding children's attention to a particular animal in the visual scene. The use of looking to visual stimuli, rather than object-choice responses, reduces the task demands of procedures requiring more complex responses such as reaching or pointing and yield continuous rather than categorical measures of attention on every trial, capturing differences in processing that might not be detected by offline tasks. This feature is important for answering questions about whether speech processing would be different from the other auditory stimuli in our task.

A second major goal of this research is to investigate how young children learn to disambiguate non-linguistic sounds, linking them to animate objects in the visual scene. Would two-year-olds map a novel animal vocalization to an unfamiliar animal? Typically, word learning is portrayed as an intractable challenge, while associating animals with the sounds they produce might appear trivial. The acoustic structure of vocalizations is influenced by the size and shape of the vocal tract and other physical features, linking sounds to their source in a non-arbitrary way. And the fact that many animal vocalizations are accompanied by synchronous physical movements might provide children with additional non-arbitrary cues to the source of the sound. Even in the absence of additional visual cues, it is often possible to pinpoint the source of a sound with reasonable accuracy. In contrast, because the acoustic structure of a word is in most cases arbitrary concerning potential referents, and it is produced by a speaker and not by the object itself, learning to associate speech sounds with objects presents a complex problem of induction [@markman1991whole]. 

One solution to the word-learning puzzle is to reduce referential ambiguity in the learning task by constraining the possible meanings of a novel word. One widely-studied constraint is that children expect words to map onto a single concept [@markman1991whole; @diesendruck2001children]. Evidence for this bias comes from experiments in which children hear a novel label in the presence of a novel object and one or more familiar objects and tend to select the novel object as the referent for the novel word. The debate about the origins, scope, and generality of this constraint has focused on whether this behavior provides evidence for a lexical constraint or whether it results instead from pragmatic inferences about a speaker's communicative intent. Under a lexical account, the constraint emerges via a "domain specific mechanism specific to word leaning" [@de2011mutual; @scofield2007two]. In contrast, pragmatic accounts propose that disambiguation behaviors should arise in any communicative context, reflecting assumptions that speakers are cooperative and use conventional names to refer to familiar objects [@bloom2002children; @clark1990pragmatics]. A third possibility is that the bias toward one-to-one mappings reflects general tendencies to find simple regularities in complex domains, a perspective embraced by recent computational approaches to word learning [@frank2009using; @mcmurray2012word; @regier2003emergent].

Thus, lexical and pragmatic accounts of the scope of children's disambiguation behavior predict that one-to-one biases are unique to word learning or that they generalize to communicative acts more broadly, while domain-general accounts predict that they would apply to any domain in which consistent one-to-one mappings are observed. To explore the possibility that one-to-one biases in sound-object mappings are not limited to interpreting communicative acts, we investigated whether children would show disambiguation behavior in a domain that is neither linguistic nor communicative, but in which consistent associations are observed between objects and auditory cues: animal vocalizations. 

The third goal of this research is to explore the link between children's disambiguation behavior and their retention of different types of sound-object associations. Recent theoretical accounts of word learning have emphasized the conceptual distinction between situation-time behaviors (figuring out the referent of a word) and developmental-time processes (slowly forming stable mappings between words and concepts) [@mcmurray2012word]. Moreover, empirical work suggests that these two constructs should not be conflated. For example, @horst2008fast found that children showed little evidence of remembering the names of novel objects they had previously identified via disambiguation after just a five-minute delay. And, using a looking-time task, @bion2013fast replicated these findings in a study with 18-, 24-, and 30-month-old infants using looking time measures. In Experiment 2, we explore this question and include measures of retention to ask whether successful disambiguation leads to recall of novel animal-vocalization associations over a short delay.

# Experiment 1

Experiment 1 asks two questions. First, can 32-month-olds use familiar animal names (e.g., dog), onomatopoeic words (e.g., bow-wow), and animal vocalizations (e.g., dog barking) to identify familiar animals? These three sound types differ in arbitrariness along a continuum, with speech as the most arbitrary and vocalizations as the least arbitrary cue. We ask whether these sounds are equally effective as acoustic cues in guiding children's attention to animals in a visual scene. Children will hear a sound cue while looking at images of two familiar animals, one that matches and one that does not match the cue, and we will compare children's looking to the matching animal when hearing the target sound. 

One of three patterns of results is most likely to emerge: The first is that children are faster to identify animal names than onomatopoeic words, and faster to identify onomatopoeic sounds than animal vocalizations. This pattern of results could be predicted by computational models that use frequency as a crucial determinant of processing speed (e.g., @mcmurray2012word). That is, children in urban environments are more likely to hear the names of animals than their vocalizations, resulting in more practice interpreting speech (high SES children might hear thousands of words daily, but not nearly as many animal sounds). This pattern of results could also be predicted by developmental accounts privilege language cues early development -- either for getting children's attention  [@vouloumanos2004tuned; @vouloumanos2007listening; @vouloumanos2007voice] or for the fact that words refer to objects directly [@waxman2009early] and might evoke more category-diagnostic features compared to environmental sounds [@edmiston2015makes]. 

The second possible pattern of results is that children are faster to identify animal vocalizations than onomatopoeic sounds, and faster to identify onomatopoeic sounds than animal names. This pattern of results could be predicted by accounts that propose that non-arbitrary sounds link directly to semantic representations, while words first activate lexical representations before reaching semantics [@chen2011crossmodal; @chen2013time]. The fact that children have experience interpreting environmental sounds (e.g., balls bouncing, things falling) before learning to interpret speech referentially could also predict an advantage for environmental sounds. A third possibility is that children are equally efficient in exploiting these three sound types to guide their attention to a familiar animal. This pattern of results would parallel that of previous studies that showed little difference in the processing of environmental sounds and words [@cummings2009infants].

Our second question is whether two-year-olds make similar inferences when mapping a novel name and a novel animal vocalization to an unfamiliar animal. In our task, children will hear a novel animal name or vocalization (instead of a familiar one), while looking at the picture of a familiar and a novel animal (instead of two familiar objects). We compare children's proportion of looking to the novel animal when hearing one of these two sound cues. 

We predict that children will look at a novel animal when hearing a novel name. When hearing a novel vocalization, one of two patterns of results is most likely to emerge: Children show no looking preference. This result would be consistent with lexical accounts that predict disambiguation only within the domain of linguistic stimuli, or by pragmatic accounts that predict disambiguation only within communicative contexts. In contrast, children could look at a novel animal when hearing a novel vocalization, with comparable performance across these two cues. This result would be consistent with accounts that propose that disambiguation biases emerge from domain-general mechanisms that look for regularities in the input.

## Method

```{r filter-e1}
d_e1 <- filter(d, experiment == "e2")
```

```{r make-e1-demo-table}
participants_e1 <- d_e1 %>% 
  select(Sub.Num, CDIAge, Sex, Months) %>% 
  unique() %>% 
  group_by(Sex) %>% 
  summarise(n = n(),
            m_age = mean(Months) %>% round(digits = 2))
```

### Participants

Participants were `r participants_e1$n %>% sum()` 32-month-old children (M=`r mean(d_e1$Months)`; range = `r range(d_e1$Months)[1]`,`r range(d_e1$Months)[2]`, `r participants_e1$n[1]` girls. All were reported by parents to be typically developing and from families where English was the dominant language. Two participants were excluded due to fussiness. Children were from mid/high-SES families.

```{r stimuli-e1, fig.pos="tb", out.width="70%", fig.align="center", fig.cap = "Trial types in Experiments 1 and 2 organized by type of cue: Familiar vs. Novel. The target animal for each trial type is on the left."}

png::readPNG(here("paper/figs", "stimuli.png")) %>% grid::grid.raster()
```

### Visual stimuli 

The visual stimuli included pictures of four Familiar animals (horse, dog, cow, sheep) and two Novel animals (pangolin, tapir).  According to parental report, the familiar animals were known by all children. Parents also reported that the novel animals were completely unfamiliar. Each animal picture was centered on a grey background in a 640 x 480 pixel space

### Auditory stimuli

The auditory stimuli consisted of sounds that were either familiar or novel to 32-month-olds. Figure 1 shows the different sound types. The familiar trials consisted of one of three different sounds: names (horse, dog, cow, and sheep), onomatopoeic words (neigh, woof-woof, moo and baa), and vocalizations (horse neighing, dog barking, cow mooing and sheep baaing). The novel sounds were used in disambiguation trials and consisted of one of two types of sounds: names (capa, nadu) and vocalizations (rhino grunting, gorilla snorting).

Trials in which the auditory cue was a familiar or novel animal name (e.g., Where’s the dog?) or a familiar or novel lexical sound (Which one goes woof-woof?) began with a brief carrier frame. The duration of the target cue was 810 ms for lexical sounds and 750 ms for animal names. The intensity of the phrases was normalized using Praat speech analysis software (Boersma, 2002).

Trials animal vocalizations began with a single word, used to draw children’s attention (e.g., Look! “dog barking”). Familiar animal vocalizations were selected based on prototypicality. After selecting at least three vocalizations for each familiar animal, the authors voted on the one that we thought would be most easily recognized by children. Choosing the novel animal vocalizations was more challenging. A group of research assistants selected from different websites several vocalizations that they judged as unfamiliar. From these vocalizations, we selected two (i.e., rhino grunting and gorilla snorting) that we judged were equally likely to be produced by the six familiar and two novel animals based on their size and vocal tract characteristics. 

We counterbalanced the vocalizations that were paired with the two novel animals, to control for the possibility that children judged one of the two novel animals as more likely to produce one of the novel vocalizations. All children were reported by parents to have had no exposure to the novel animal’s natural vocalizations.  The duration of the target animal vocalizations was 2000 ms. 

### Familiarization books

Since we were working with children from mid/high-SES families growing up in an urban environment, we were concerned that they would not be familiar with many animal vocalizations or onomatopoeic words. So to ensure that all children had at least some experience with the familiar animals and auditory cues in our study, we sent parents two children's books, both titled Sounds on the Farm, a week before their visit. Parents were instructed to share each book with their child for 5 to 10 min on at least three days before the experiment. The first book consisted of colorful pictures of each familiar animal and text designed to prompt parents to produce each animal’s lexical sound (e.g., Wow, look at all those cows! This cow says moo, moo!). To give children exposure to the natural animal vocalizations, we used a Hear and There book, which contained buttons that children could press to hear the actual noise that each animal produces.

We thought this design decision was important for two reasons. First, it would provide more confidence that differences in children's performance within familiar trials would be related to processing speed and not driven by lack of exposure to one of the sound types. Second, the association between familiar vocalizations and animals is a prerequisite for success on disambiguation, a key question for our experiments. 

### Procedure 

We used the looking-while-listening (LWL) procedure (see Fernald et al., 2008) to measure differences in processing familiar sounds. Previous studies have shown that even when objects are reported by parents as familiar to their children, or when children are at ceiling in offline reaching tasks, these real-time processing measures can capture meaningful differences in processing. These differences correlate to properties of the sound stimuli (e.g., word-frequency) and different aspects of the child's experience (e.g., their age, socioeconomic status, amount of parental talk). Looking-time measures have also been used in disambiguation tasks with children from different ages, capturing differences in accuracy that relate to children's age and vocabulary size (Bion et al., 2013). 

On each trial, a pair of pictures was presented on the screen for approximately 4 s, with the auditory stimuli starting after 2 s, followed by 1 s of silence. As seen in Figure 1, we have two main trials types, familiar and disambiguation, paralleling our original two research questions on children's processing of familiar and novel auditory cues.

There were three types of familiar trials: name, onomatopoeic word, and vocalization. On 8 Name trials, each familiar animal served as the target twice and was paired once with another familiar animal and once with a novel animal. On 8 Onomatopoeic-word trials, each familiar animal served as the target twice. On 16 Vocalization trials, each familiar animal served as the target four times, paired twice with another familiar animal and twice with a novel animal. These three familiar sound types should allow us to answer our first research question: whether names, onomatopoeic words, or animal vocalizations, are equally effective as acoustic cues in guiding children's attention to animals in a visual scene.

There were two types of disambiguation trials: names and vocalizations. On 6 Name trials, each novel animal was labeled three times with a novel animal name, always paired with a familiar animal. On 8 Vocalization trials, each novel animal vocalization served as the target four times and was paired with each familiar animal once. These two sound types should allow us to answer our second research question: whether two-year-olds make similar inferences when mapping a novel name and a novel animal vocalization to an unfamiliar animal.

There were two different visits. The familiar and disambiguation trials with animal names and onomatopoeic words were administered during visit one. The Familiar and Disambiguation Trials with the animal vocalizations were administered during their second visit. We administered the animal vocalizations on the second visit to allow children to become familiar with the procedure and to give parents additional time to use the familiarization books. 

During each visit, five filler trials were interspersed throughout to add variety and maintain children’s attention. Pairings of the novel animal and name and side of presentation of target animals were counterbalanced across participants. Caregivers wore darkened sunglasses so that they could not see the pictures and influence infants’ looking throughout the 5-min procedure.

### Measures of processing efficiency

Participants' eye movements were video-recorded and coded with a precision of 33 ms by observers who were blind to trial type. For 25% of the subjects, two measures of inter-observer reliability were assessed: the proportion of frames on which two coders agreed (98%) and timing of shifts in gaze, ignoring steady-state fixations (94%).

**Accuracy:** On those trials in which the infant was fixating a picture at the onset of the speech stimulus, accuracy was computed by dividing the time looking to the target object by the time looking to both target and distracter, from 300 to 2500 ms from the onset of the target sound. We chose this window because shifts to the target occurring before 300 ms were likely initiated before the onset of the noun and the animal vocalizations were on average two seconds in duration. We used a single window for all trial types and computed mean accuracy for each participant on each trial type.

**Reaction time:** We calculated reaction time (RT) on those trials on which participants were looking at the distractor animal at the beginning of the sound. RT on each trial was the latency of the first shift to the correct animal within a 300- to 1,800-ms window from sound onset, as typically done in studies using this procedure [@fernald2008looking].

## Results and discussion

### Using familiar animal names, onomatopoeic sounds, and vocalizations to identify familiar animals:

Our first question is whether a familiar animal name, onomatopoeic word, and animal vocalization are equally effective in guiding children's attention to an animal in the visual scene. Figure 2A shows children's looking behavior over time on the LWL procedure [@fernald1998rapid]. To capture children's speed of processing, we show children's responses on trials in which they start looking at the wrong animal. From sound onset onward, we show the mean proportion of trials in which children were looking at the correct picture, every 33ms, with different lines representing children's responses on Name trials (black), Onomatopoeic-word trials (light grey), and Vocalization trials (dark grey). The y-axis shows the mean proportion of trials on which children were looking at the correct animal. The x-axis represents time from sound onset in milliseconds. Around 750ms from sound onset there is already a substantial difference in the mean proportion of trials in which children are looking at the correct animal depending on whether they heard an animal name or an animal vocalization. Children are looking at the correct animal in a greater proportion of trials when they hear a familiar animal name, as compared to when they hear an animal vocalization, with performance on trials with onomatopoeic words falling between the two.

```{r filter-model-rt-e1}
d_model_e1_rt <- d_models$rt_e1 
```

```{r e1-rt-model-summary}
e1_model_summary_rt <- d_model_e1_rt %>% 
  group_by(cue_type) %>% 
  summarise(m = mean(rt_scale_param),
            hdi_lower = quantile(rt_scale_param, probs = 0.025),
            hdi_upper = quantile(rt_scale_param, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2)
```

```{r e1-rt-group-diffs}
e1_rt_hyp_d <- d_model_e1_rt %>% 
  select(-param_est) %>% 
  spread(key = cue_type, value = rt_scale_param) %>% 
  mutate(name_onomatopoeic_diff = name - onomatopoeia,
         name_vocalization_diff = name - vocalization,
         onomatopeic_vocalization_diff = onomatopoeia - vocalization) %>% 
  select(-name, -onomatopoeia, -vocalization) %>% 
  gather(key = ind_contrast, value = param_est, -sample_id) 

e1_rt_hyp_table <- e1_rt_hyp_d %>% 
  group_by(ind_contrast) %>% 
  summarise(m = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2)

prob_diff0 <- e1_rt_hyp_d %>% 
  group_by(ind_contrast) %>% 
  summarise(prob = mean(param_est < 0)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 4)
```

To quantify these differences, we fit Bayesian mixed-effects regression models using the `rstanarm` [@gabry2016rstanarm] package in `r my_citations$r`. The mixed-effects approach allowed us to model the nested structure in our data by including random intercepts for each participant and item and a random slope for each item. We used Bayesian methods to quantify support in favor of null hypotheses of interest – in this case, the absence of a difference in real-time processing across the different familiar cue types. To communicate the uncertainty in our estimates, we report the 95% Highest Density Interval (HDI) around the point estimates of the group means and the difference in means. The HDI provides a range of plausible parameter values given the data and the model. All analysis code is available in the online repository for this project: https://github.com/kemacdonald/anime.

We computed reaction time (RT) as the mean time it took them to shift to the correct picture on trials in which they were looking at the wrong picture at sound onset for the three cue types. To make RTs more suitable for modeling on a linear scale, we analyzed responses in log space using a logistic transformation, with the final model was specified as: $log(RT) \sim cue\_type + (1 + sub\_id \mid item)$. 

Figure 2 shows the data distribution for each participant's RT (2B), the estimates of condition means, and the full posterior distribution of condition differences across the different cue types (2C). Children were faster to identify the target animal while hearing its name ($M_{name}$ = `r e1_model_summary_rt$m[3]` ms), as compared to its onomatopoeic animal sound ($M_{onomatopeia}$ = `r e1_model_summary_rt$m[1]` ms), and its vocalization ($M_{vocalization}$ = `r e1_model_summary_rt$m[2]` ms). The difference between RTs for the name and onomatopoeic animal sounds was `r e1_rt_hyp_table$m[1]` ms with a HDI from `r e1_rt_hyp_table$hdi_lower[1]` ms to `r e1_rt_hyp_table$hdi_upper[1]` ms. While the null value of zero difference falls within the 95% HDI, `r prob_diff0$prob[1] * 100`% of the credible values fall below the null, providing some evidence for faster processing of animal names. The average difference in children's RT between the name and vocalization trials was `r e1_rt_hyp_table$m[2]` ms with a HDI from `r e1_rt_hyp_table$hdi_lower[2]` ms to `r e1_rt_hyp_table$hdi_upper[2]` ms and `r prob_diff0$prob[2] * 100`% of the credible values falling below zero, providing strong evidence that children processed names more efficiently as compared to vocalizations. Finally, the average difference in children's RT between the onomatopoeic sounds and vocalization trials was `r e1_rt_hyp_table$m[3]` ms with a HDI from `r e1_rt_hyp_table$hdi_lower[3]` ms to `r e1_rt_hyp_table$hdi_upper[3]` ms and `r prob_diff0$prob[3] * 100`% of the credible values falling below zero. 

Together, the RT modeling results provide strong evidence that children processed animal names around `r  e1_rt_hyp_table$m[2] %>% round(digits = 0) %>% abs()` ms faster than animal vocalizations, with almost all of the estimates of the plausible RT differences falling below the null value of zero. There was slightly weaker evidence that children processed animal names more efficiently compared to onomatopoeic animal sounds but strong evidence of faster processing of onomatopoeic animal sounds compared to animal vocalizations. In sum, there was evidence of a graded effect of cue type on RTs with names being faster than onomatopoeic animal sounds, which were faster than animal vocalizations.

```{r oc-plot-e1, fig.pos="t", fig.align='center', out.width = "95%", fig.cap="Reaction Time (RT) results for Experiment 1. Panel A shows the timecourse of children’s looking to the target animal after hearing a familiar animal name (black), onomatopoeic word (light grey), or animal vocalization (dark grey). Panel B shows the distribution of RT data across conditions. Each grey point shows the average RT for a single participant. The black box represent the most likely estimate of the condition means. Panel C shows the posterior distribution of plausible RT differences for each pair-wise comparison. Color represent the contrast. The black vertical dashed line represents the null value of zero condition difference. All error bars represent 95\\% Highest Density Intervals."}

png::readPNG(here("paper/figs", "oc_e1_new.png")) %>% grid::grid.raster()
```

```{r filter-model-acc-e1}
d_model_acc_e1 <- d_models$acc_e1 %>% 
  mutate(condition = str_c(cue_type, trial_type, sep = "_")) %>% 
  filter(condition != "onomatopoeic_novel") 
```

```{r summarise-model-acc-e1}
# summarize posterior distributions for accuracy
e1_model_summary_acc <- d_model_acc_e1 %>% 
  group_by(cue_type, trial_type) %>% 
  summarise(m = mean(acc_prob_scale),
            hdi_lower = quantile(acc_prob_scale, probs = 0.025),
            hdi_upper = quantile(acc_prob_scale, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) 
```

```{r summarise-group-comparisons-acc-fam-e1}
e1_acc_hyp_fam_d <- d_model_acc_e1 %>% 
  filter(trial_type == "familiar") %>% 
  select(-param_estimate, -condition) %>% 
  spread(key = cue_type, value = acc_prob_scale) %>% 
  mutate(name_onomatopoeic_diff = onomatopoeic - name,
         name_vocalization_diff = vocalization - name,
         onomatopeic_vocalization_diff = vocalization - onomatopoeic) %>% 
  select(ends_with(match = "diff"), sample_id) %>% 
  gather(key = ind_contrast, value = param_est, -sample_id) 

e1_acc_hyp_fam_table <- e1_acc_hyp_fam_d %>% 
  group_by(ind_contrast) %>% 
  summarise(m = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2)

null_value_acc_diff <- 0
null_value_acc_chance <- 0.5

prob_diff_chance_e1_acc_fam <- e1_acc_hyp_fam_d %>% 
  group_by(ind_contrast) %>% 
  summarise(prob = mean(param_est < null_value_acc_chance)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 4)
```

Next, we estimated children's attention to the target image over the course of the trial to ask whether the three trial types were equally effective cues to guide their attention to a familiar animal.  The upper left panel of Figure 3A shows children's proportion looking to the target for each trial type.  Visual inspection of the plot suggests two things: (1) children reliably looked to the correct animal after hearing each of the three familiar cues and (2) children's overall looking behavior was strikingly similar across conditions.

Next, we quantified the strength of evidence for the absence of any condition differences. We estimated the mean proportion looking for each trial type using a Bayesian linear mixed-effects model with the same specifications as the RT model described above. We transformed the proportion looking scores using the empirical logit, with the final model as: $logit(accuracy) \sim cue\_type + (1 + sub\_id \mid item)$. The black boxes in Figure 3A show mean Accuracy for each familiar cue type ($M_{name}$ = `r e1_model_summary_acc$m[1]`, $M_{onomatopeia}$ = `r e1_model_summary_acc$m[3]`, and ($M_{vocalization}$ = `r e1_model_summary_acc$m[4]`). Children's looking to the target image was reliably different from a model of random looking across all conditions, with the null value of 0.5 falling well outside of the range of plausible values (see the difference between the horizontal dashed line and error bars in Figure 3A). 

Moreover, the three cues types were equally effective in guiding children’s attention to the target animal over the course of the trial as shown by the high overlap in the posterior distributions of the accuracy measure with the null value of zero difference falling within the 95% HDI for all group comparisons (name vs. onomatopoeia: $\beta_{diff}$ = `r e1_acc_hyp_fam_table$m[1]`, 95% HDI from `r e1_acc_hyp_fam_table$hdi_lower[1]` to `r e1_acc_hyp_fam_table$hdi_upper[1]`; name vs. vocalization: $\beta_{diff}$ = `r e1_acc_hyp_fam_table$m[2]`, 95% HDI from `r e1_acc_hyp_fam_table$hdi_lower[2]` to `r e1_acc_hyp_fam_table$hdi_upper[2]`; onomatopoeia vs. vocalization: $\beta_{diff}$ = `r e1_acc_hyp_fam_table$m[3]`, 95% HDI from `r e1_acc_hyp_fam_table$hdi_lower[3]` to `r e1_acc_hyp_fam_table$hdi_upper[3]`). These results provide evidence that the animal vocalizations were equally effective at directing looking behavior if children have enough time to process the cue.

### Using novel animal names and animal vocalization to disambiguate novel animals

```{r summarise-model-acc-e1-novel}
e1_acc_hyp_nov_d <- d_model_acc_e1 %>% 
  filter(trial_type == "novel") %>% 
  select(-param_estimate, -condition) %>% 
  spread(key = cue_type, value = acc_prob_scale) %>% 
  mutate(name_vocalization_diff = vocalization - name) %>% 
  select(ends_with(match = "diff"), sample_id) %>% 
  gather(key = ind_contrast, value = param_est, -sample_id) 

e1_acc_hyp_nov_table <- e1_acc_hyp_nov_d %>% 
  group_by(ind_contrast) %>% 
  summarise(m = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2)

null_value_acc_diff <- 0
null_value_acc_chance <- 0.5

prob_diff_chance_e1_acc_nov <- e1_acc_hyp_nov_d %>% 
  group_by(ind_contrast) %>% 
  summarise(prob = mean(param_est < null_value_acc_chance)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 4)
```

Our next question is whether children would orient to a novel animal after hearing a novel animal name or vocalization, thus showing evidence of one-to-one biases for the vocalizations that animals produce. We focus on children's accuracy, comparing their proportion of looking to the novel animal against chance performance and across cue types.  The orange points in Figure 3A show Accuracy group means for each novel cue type ($M_{name}$ = `r e1_model_summary_acc$m[2]`, $M_{vocalization}$ = `r e1_model_summary_acc$m[5]`). Children's looking to the target image was reliably different from a model of random looking behavior across both cue types, with the null value of $0.5$ falling well outside of the range of plausible values. 
Moreover, the novel animal name and vocalizations were equally effective in guiding children’s attention to the target animal over the course of the trial (name vs. onomatopoeia: $\beta_{diff}$ = `r e1_acc_hyp_nov_table$m[1]`, 95% HDI from `r e1_acc_hyp_nov_table$hdi_lower[1]` to `r e1_acc_hyp_nov_table$hdi_upper[1]`). This result provides converging evidence that the animal vocalizations guided children's attentin in simlar ways if children have enough time to process the cue.

```{r acc-plot-e1, fig.pos="t", fig.align='center', out.width = "85%", fig.cap = "Accuracy results for Experiment 1 for familiar (left) and novel (right) trials. Each point represents the proportion target looking for a single participant. All other plotting conventions are the same as in Figure 2."}

png::readPNG(here("paper/figs", "prop_look_e1_new.png")) %>% grid::grid.raster()
```

Therefore, children appear to show one-to-one biases for the vocalizations that animals produce already at 32 months of age, the earliest age at which the disambiguation effect has been observed in a domain other than word learning. There were no significant differences between children's reaction times for novel animal names or vocalizations.

# Experiment 2

One issue that has received much attention in recent years concerns the relation between children’s referent selection and retention abilities. While earlier studies tended to conflate disambiguation strategies and children’s word learning, more recent studies suggest that these two abilities should not be conflated [@bion2013fast; @horst2008fast]

@horst2008fast examined both referent selection and retention in four experiments with 2-year-olds.  When children were shown a novel object among familiar objects, they selected the novel object when hearing a novel label, as found in previous studies. But surprisingly, on retention trials 5 min later, these children showed no evidence of remembering the names of the novel objects they had previously identified. Using a looking-time task, @bion2013fast replicated these findings in a study with 18-, 24-, and 30-month-old infants using looking time measures of performance.

Experiment 2 asks whether children can retain the link created through disambiguation between a novel animal and a novel animal vocalization. We also aim to replicate the findings from Experiment 1, showing that children can identify familiar animals based on the vocalizations they produce and use novel vocalizations to disambiguate novel animals. We predict that children will succeed in disambiguation trials, but will show little evidence of retention on subsequent disambiguation trials, paralleling the findings of earlier studies with linguistic stimuli.

## Method

### Participants

```{r participant-demo-e2}
d_e2 <- filter(d, experiment == "e2")

participants_e2 <- d_e2 %>% 
  select(Sub.Num, CDIAge, Sex, Months) %>% 
  unique() %>% 
  group_by(Sex) %>% 
  summarise(n = n(),
            m_age = mean(Months) %>% round(digits = 2))
```

Participants were `r sum(participants_e2$n)` 31-month-old children (M = `r d_e2$Months %>% mean(na.rm=T)` months; range = `r d_e2$Months %>% min()` - `r d_e2$Months %>% max()`), `r participants_e2$n[1]` girls. All children were typically developing and from families where English was the dominant language.

### Visual stimuli  

The visual stimuli were the same as in Experiment 1, except for the novel animals (aardvark and capybara), which replaced the novel animals (pangolin and tapir) used in Experiment 1 (see animals in Figure 4). We decided to change the novel animals to test whether our results would generalize beyond the particular stimulus set in Experiment 1. All children were reported by parents to have had no exposure to the novel animals.

### Auditory stimuli  

The auditory stimuli consisted of the same familiar and novel animal vocalizations as in Experiment 1.

```{r stimuli-e2, fig.pos="t", out.width="70%", fig.align="center", fig.cap = "Trial types in Experiments 4 organized by type of trial. Children hear familiar and novel vocalizations. The target animal for each trial type is on the left."}

png::readPNG("figs/stimuli_e2.png") %>% grid::grid.raster()
```

### Familiarization books 

As in Experiment 1, we sent home a children’s book to ensure that all participants had at least some exposure to the familiar animals and auditory cues. Since we were interested in the natural animal vocalizations and not the names/lexical sounds, we only sent the Hear and There Sounds on the Farm book.  Instructions were the same as Experiment 1, and the book was sent a week before the visit.

### Procedure

Experiment 2 consisted of one visit. Each child saw 30 trials, consisting of three trial types (Figure 4). The 16 familiar trials and eight disambiguation trials were identical to the vocalization trials in Experiment 1. On six retention trials, the two novel animals were presented side by side, with each serving as the target three times.  The same coding and speed/accuracy measures were used as in Experiment 1.

## Results and discussion

```{r e2-model-filter}
d_model_acc_e2 <- d_models$acc_e2 
```

### Retention of the link between a novel animal and a novel vocalization:

```{r e2-acc-model-summary}
# summarize posterior distributions for accuracy
e2_model_summary_acc <- d_model_acc_e2 %>% 
  group_by(trial_type) %>% 
  summarise(m = mean(acc_prob_scale),
            hdi_lower = quantile(acc_prob_scale, probs = 0.025),
            hdi_upper = quantile(acc_prob_scale, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) 
```

```{r e2-acc-group-comparison}
e2_acc_hyp_d <- d_model_acc_e2 %>%
  select(-param_estimate) %>% 
  spread(key = trial_type, value = acc_prob_scale) %>% 
  mutate(fam_disambig_diff = familiar - disambiguation,
         fam_reten_diff = familiar - retention,
         disambig_reten_diff = disambiguation - retention) %>% 
  select(ends_with(match = "diff"), sample_id) %>% 
  gather(key = ind_contrast, value = param_est, -sample_id) 

e2_acc_hyp_table <- e2_acc_hyp_d %>% 
  group_by(ind_contrast) %>% 
  summarise(m = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2)
```

Figure 5A shows children’s proportion looking to the target animal after hearing a familiar or a novel animal vocalization over the same analysis window used in Experiment 1 (300 to 2500 ms). Visual inspection of the figure suggests that children successfully oriented to the target image after hearing both familiar and novel animal vocalizations. The black boxes show mean proportion target looking for familiar ($M_{familiar}$ =  `r e2_model_summary_acc$m[2]`, disambiguation ($M_{disambiguation}$ =  `r e2_model_summary_acc$m[1]`, and retention trials ($M_{retention}$ = `r e2_model_summary_acc$m[3]`). 

Children's looking to the target image was reliably different from random-looking behavior for both familiar and disambiguation trials, with the null value of `r null_value_acc_chance` falling well outside of the range of plausible values. Moreover, the novel animal vocalizations and the familiar animal vocalizations were equally effective in guiding children’s attention to the target animal over the course of the trial (familiar vs. disambiguation: $\beta_{diff}$ = `r e2_acc_hyp_table$m[2]`, 95% HDI from `r e2_acc_hyp_table$hdi_lower[2]` to `r e2_acc_hyp_table$hdi_upper[2]`). 

In contrast to children's success on familiar and disambiguation trials, they did not show evidence of retaining the link between the novel animal and the novel animal vocalization, with the null value of $0.5$ proportion looking falling well within the range of plausible estimates. Moreover, there was strong evidence that children were less accurate on retention trials compared to both disambiguation trials ($\beta_{diff}$ = `r e2_acc_hyp_table$m[1]`, 95% HDI from `r e2_acc_hyp_table$hdi_lower[1]` to `r e2_acc_hyp_table$hdi_upper[1]`) and familiar trials ($\beta_{diff}$ = `r e2_acc_hyp_table$m[3]`, 95% HDI from `r e2_acc_hyp_table$hdi_lower[3]` to `r e2_acc_hyp_table$hdi_upper[3]`).

```{r acc-plot-e2, fig.pos="t", fig.align='center', out.width = "85%", fig.asp = 0.8,, fig.cap= "Proportion target looking for familiar and novel animal vocalizations in Experiment 2.  All  plotting conventions are the same as in Figures 2 and 3."}

png::readPNG(here("paper/figs", "prop_look_e2_new.png")) %>% grid::grid.raster()
```

Three findings emerged from the accuracy analysis: First, children oriented to a familiar animal after hearing a familiar animal vocalization. Second, children oriented to a novel animal after hearing a novel animal vocalization. These two results are an internal replication of the key findings from Experiment 1 in a new sample. Also, we found that children did not show evidence of retaining the link between a novel animal vocalization and a novel animal.

# General Discussion

Three main findings emerged from this work. First, 32-month-olds responded fastest to a familiar animal name and slowest to a familiar animal vocalization, with onomatopoeic sounds falling in between. Children could, however, identify the familiar animals after hearing any of on these three sound types. The second finding was that children showed disambiguation biases for the types of vocalizations that animals produce, similar to their biases in word learning and communicative contexts. The third finding was that these biases do not necessarily lead to learning, as children were not able to retain the link between novel animals and their vocalizations. This lack of retention parallels the findings of recent word-learning studies [@mcmurray2009integrating; @bion2013fast; @horst2008fast] and emphasizes the theoretical importance of disentangling processes of disambiguating reference, an in-the-moment phenomenon, and word learning, which occurs over a longer timescale.

We found a processing speed advantage for words over other meaningful sounds. Some theories of language development argue that words are unique stimuli because they refer to objects in the world [@waxman2009early], while other theories argue that words are special because they activate conceptual information more quickly, accurately, and in a more categorical way than nonverbal sounds [@edmiston2015makes]. It is also possible that words and nonverbal sounds might be processed by different brain regions, with words being accessed more rapidly. A second explanation for the advantage for words might be differences in sheer frequency in the input. At least in our sample, it is safe to assume that children have heard the word cow many more times than they have heard an actual cow mooing. Frequency effects have been robustly demonstrated in the processing of words, with adults being faster to recognize words that they hear more frequently [@dahan2001time]. A final explanation is that words are very effective at presenting a lot of information in a short period. When children see a simple visual world consisting of a dog and a sheep, the first phoneme of the target word is already sufficient to determine the animal that is likely to be talked about next.

Much less is known about children’s and adults’ processing of onomatopoeic sounds. @hashimoto2006neural compared brain responses to nouns, animal sounds, and onomatopoetic sounds, and found that onomatopoeic sounds were processed by extensive brain regions involved in the processing of both verbal and nonverbal sounds. @cummings2009infants argues that onomatopoeic sounds might provide young children with information about intermodal associations, bridging their understanding of non-arbitrary environmental sounds and arbitrary word-object associations. @fernald1993common reported that 52% of Japanese mothers used onomatopoeic sounds to label target objects, while only 1 in 30 American mothers did so (see also @laing2014phonological). While our results do not speak directly to these theoretical issues, they do suggest that onomatopoeic sounds function like words in that they are capable of activating conceptual representations that drive children's visual attention to seek the physical referent of the sound (see also @peeters2016processing for converging evidence with adults). There was, however, some evidence that children processed onomatopoeic sounds less efficiently compared to words in our task. This finding contrasts with the results of @laing2017perceptual study where they reported an onomatopoeic processing advantage over words in 10-month-old infants. It is interesting to consider that developmental changes in the relative processing speed of these two cues could be driven by accumulating differences in the frequency of words and onomatopoeia in children's input. 

Our second finding was that children looked at a novel animal when hearing a novel animal vocalization, with accuracy comparable to their disambiguation of novel animal names. @bloom2002children outlines three different theories that could explain children’s disambiguation biases. These biases could be a specifically lexical phenomenon that applies only to words (lexical account), a product of children’s theory of mind restricted to communicative situations (pragmatic account), or a special case of a general principle of learning that exaggerates regularities across domains (domain-general account). By using animal sounds, this study provides an important empirical result since the theoretical accounts make different predictions for children's looking behavior in response to a stimulus that is non-linguistic and non-communicative.

Previous studies contrasted lexical-specific and pragmatic accounts. For example, @diesendruck2001children found that children expect speakers to use consistent facts to refer to objects, and they select a novel object when hearing a novel fact. Recent studies suggest that different strategies might be used to make inferences about speakers’ communicative intent and the meaning of a novel word. Autistic children who are known to have pragmatic deficits show disambiguation biases and select a novel word when hearing a novel object [@preissler2005role]. Moreover, disambiguation biases for words are correlated with vocabulary, and disambiguation biases for facts are correlated with social-pragmatic skills [@de2011mutual]. These findings suggest that disambiguation biases for words might not be motivated uniquely by pragmatic inferences, but they do not provide evidence for or against domain-general accounts of the disambiguation bias.

Few studies have looked at disambiguation biases in non-linguistic domains. @moher2010one showed that three-year-olds link different voices to unique faces, showing that one-to-one biases might extend to other communicative domains. However, @yoshida2012exclusion found that adults in a statistical learning task were less likely to show evidence of using disambiguation biases to learn nonspeech sounds even though this behavior would have facilitated task performance. These results suggest that at some point in development disambiguation constraints may operate more strongly over speech compared to nonspeech sounds. Critically, these results do not provide evidence against a domain-general account since participants had no reason to expect that the mapping between random non-linguistic sounds and objects should be mutually exclusive. It could be that similar learning strategies might be applied to non-linguistic sounds when they become meaningfully related to objects in the environment or relevant for communication with other people.

The work reported here demonstrates that young children do show evidence of disambiguation biases in a non-linguistic and non-communicative domain. These findings are consistent with predictions made by domain-general accounts that explain disambiguation biases as the byproduct of a system that attempts to find regularities in complex learning tasks that involve consistent mappings. Previous Connectionist and Bayesian models of word learning showed that disambiguation biases emerge without built-in constraints on the types of meanings words could have [@frank2009using; @mcmurray2012word; @regier2003emergent]. In principle, these same biases could emerge if these models attempted to map animal vocalizations to animals and there were consistent co-occurrences present in the learning environment.

One open question is whether a single disambiguation mechanism can account for the diverse set of contexts in which children show disambiguation behaviors. Here, we found that children could disambiguate stimuli other than words and facts, suggesting at least the existence of a domain-general mechanism that leads to disambiguation. A preference for parsimony might favor a single mechanism. But as pointed out by recent computational work, it is possible that different mechanisms jointly contribute to disambiguation behavior, explaining the diverse set of contexts and populations in which disambiguation behavior has been observed [@lewis2013modeling]. Thus, it is possible that the same behavior - selecting a novel object when hearing a novel auditory stimulus - might result from different computational mechanisms depending on the task at hand, children's age, or the particular stimuli set and assumptions about the people involved in the interaction. Put another way, children could use a domain-general mechanism to learn about novel animal vocalizations, and they could use a lexical and pragmatic constraint to learn about novel words.

Our third finding was that one-to-one biases for animal vocalizations do not necessarily lead to retention of the link between a novel animal and a novel vocalization. This finding dovetails with recent cross-situational models of early word learning that emphasize the separate processes of disambiguating reference and learning word-object labels over time [@mcmurray2012word]. McMurray and colleagues propose that referent-selection requires that children give their best guess about a new word's meaning in a specific ambiguous situation, but that learning operates over a developmental timescale, requiring multiple exposures to build up stable word-object links. Although disambiguation can be viewed as the product of learning that has occurred up to that point, for younger children it does not necessarily result in learning. These claims are also supported by evidence from studies on early word learning using online and offline measures of retention [@bion2013fast].

These results also add to a body of work that encourages us to think differently about disambiguation biases. This work has emphasized the role of experience, showing that the tendency to select a novel object when hearing a novel word is not robustly present across populations. For example, bilingual children, children from lower socioeconomic status, children who receive less language input, and children with less structured vocabularies or smaller vocabulary sizes, take longer to show evidence of disambiguation biases [@bion2013fast; @yurovsky2012mutual]. Moreover, the current study adds to recent studies taking a fresh look at an old question: the scope of disambiguation biases [@suanda2012detailed; @suanda2013young]. Taken together, these findings suggest that it is useful to consider variability in the emergence, function, and scope of language learning processes that might be characterized as universal based on studies using a particular population or a specific stimulus type.

Finally, our results emphasize that children’s learning about objects in their environment involves more than learning their names. Before object names are learned, sounds and actions might form the basis on which objects are conceptualized. For example, children might see barking as a defining feature of dogs and may say bow-wow in response to the picture of a dog, even before they learn the animal name [@nelson1974concept]. Learning the meaning of an object, therefore, requires learning several cross-modal associations, including learning the object’s texture, smell, as well as its sounds and names. Children do not have explicit constraints that freshly baked cookies should have only one smell. Yet, they might recognize and get excited about the familiar smell coming from the kitchen and might assume their mothers are baking something new when smelling something unfamiliar.

## Conclusions

Children use different types of knowledge to make sense of a constantly changing world. They might identify animal vocalizations based on the shape of the vocal tract of the animal, its location and size, and their previous knowledge about animal vocalizations. Importantly, these cues normally converge in helping children identify an animal in the environment. The same is true for their identification of referents for words. Children can identify the referent for a word based on semantics [@goodman1998role], cross-situational statistics [@smith2008infants], syntax [@brown1957linguistic], pragmatic and social cues [@baldwin1993infants], and disambiguation biases [@markman1991whole]. As children grow older, these different sources of information provide converging evidence that a novel word should refer to a novel object. Children can rely on their knowledge about the world, speakers, and on their previous experiences with words to figure out what speakers are talking about – a task we continue to do throughout our lives when learning new words and interpreting complex sentences.


\newpage

# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}


