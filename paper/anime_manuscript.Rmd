---
title             : "Moos as cues: One-to-one mappings in a non-linguistic and non-communicative domain"
shorttitle        : "Moos as cues"

author: 
  - name          : "Kyle MacDonald"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Serra Mall, Stanford, CA 94306"
    email         : "kylem4@stanford.edu"
  - name          : "Ricardo A. H. Bion"
    affiliation   : "1"
  - name          : "Anne Fernald"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Stanford University"

author_note: |
  

abstract: |
  When hearing a novel name, children tend to select a novel object rather than a familiar one. This tendency suggests that children expect language to have one-to-one word-concept mappings. But what underlying cognitive processes explain children's disambiguation behavior? Theoretical accounts disagree, with some proposals explaining disambiguation as a result of expectations about the nature of words, while other proposals argue in favor of general expectations about speakers' communicative intent. Here, we investigated whether similar patterns of disambiguation behaviors emerge in a domain that is non-linguistic and non-communicative, but in which strong regularities can be found: the vocalizations that animals produce. Using real-time processing measures, we first show that two-year-olds can identify familiar animals based on their vocalizations, though not as fast as they are to identify these same animals when hearing their names. We then show that children tend to look at an unfamiliar animal when hearing a novel animal vocalization or novel animal name, responding with similar speed in both contexts. In Experiment 2, we replicate the key finding that children look at a novel animal when hearing a novel animal vocalization, but find that disambiguation does not necessarily lead to learning measured at a longer timescale. These results are consistent with an account of disambiguation behaviors arising from domain-general processing mechanisms.
  
keywords          : "disambiguation, mutual exclusivity, environmental sounds, retention, word learning"
wordcount         : "7923"

bibliography      : ["anime.bib", "r-references.bib"]

figsintext        : yes
figurelist        : yes
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
---

```{r load-packages, include = FALSE}
library("papaja"); library(knitr); library(png); library(here); library(tidyverse)
```

```{r global-options, include=FALSE}
knitr::opts_chunk$set(cache = T, fig.width = 5, fig.asp = 0.618, fig.align = "center")
```

```{r load-data-models}
d <- read_csv(here("data/03_summary_tables", "anime_trial_level.csv"))
d_models <- read_rds(here("data/03_summary_tables", "anime-posterior-samples.rds"))
```

```{r r-refs}
r_refs(file = "r-references.bib")
my_citations <- cite_r(file = "r-references.bib", 
                       pkgs = c("tidyverse", "rstanarm", "papaja", "here", "knitr"), 
                       withhold = FALSE,
                       footnote = TRUE)
```

# Introduction

Words are a core feature of natural languages and provide a link between communicative signals and lexical concepts. One central property of words is that they tend to correspond to single meanings. This regularity, in turn, can help novice word learners to leverage exisiting knowledge to acquire new word meanings despite the potential for an (in principle) unlimited amount of referential uncertainty ^[This problem is a simplified version of Quine's \textit{indeterminacy of reference} [@quine19600]: That there are many possible meanings for a word ("Gavigai") that include the referent ("Rabbit") in their extension, e.g., "white," "rabbit," "dinner." Quine's broader philosophical point was that different meanings ("rabbit" and "undetached rabbit parts") could actually be extensionally identical and thus impossible to tease apart.]. In fact, even young children behave as if they expect words to map onto single concepts, a behavior know as the "disambiguation" effect. But what cognitive processes underly children's tendency to disambiguate? And are these processes limited to the linguistic and communicative domains?

Consider that when children encounter people and animals in daily life, they experience them through multiple sensory modalities simultaneously. This creates a scenario where learners could be acquiring several different multimodal associations. For example, when playing with their pet, a child could learn to link the physical features and actions of a dog with its barking sound or with the onomatopoeic word resembling its characteristic vocalization, such as woof-woof. Eventually, the child will learn to associate the word "dog" with these other features. Put another way, just as familiar object labels are consistently associated with particular referents, animal vocalizations and onomatopoeic words for vocalizations also provide consistent associations between an auditory stimulus and an object in the visual world, making them a particularly good tool for answering questions about (a) the nature of the mechanisms underlying children's disambigubation biases and (b) whether words are a particularly unique auditory stimulus. In the work reported here, we compare young children’s efficiency in using words, onomatopoeia, and animal vocalizations as cues to identifying particular animals in the visual scene. We also ask whether children will show disambiguation behaviors, which have been explained via pragmatic or lexically-specific mechansims [@bloom2002children; @diesendruck2001children; @markman1991whole] to select which of two animals generated an unfamiliar animal vocalization.

The question of whether words are a special kind of stimulus is not new. Several studies have found advantages for speech sounds over tones in object individuation and categorization in young infants [@fulkerson2007words; @xu2002role]. Focusing on associations between objects and sounds, objects and tones, or objects and gestures, several studies found that younger infants accept several different forms as potential object labels, but that older infants are more discriminating and favor words [@namy1998words; @woodward1999infants]. Another line of research found that infants prefer to hear spoken words over some non-linguistic analogs [@vouloumanos2004tuned; @vouloumanos2007listening; @vouloumanos2007voice] and that the neonate brain responds differently to speech as compared to backward speech [@pena2003sounds]. These studies found advantages for speech over non-linguistic analogs in categorization, individuation, crossmodal association, and speech preferences. However, they all focused on arbitrary, non-linguistic cues that are not consistently associated with objects in children's everyday environments.
 
Other research has approached the question of whether speech is special from a different perspective, comparing how people process spoken words as compared to non-arbitrary environmental sounds, such as animal vocalizations (e.g., cat meowing) or the sounds produced by inanimate objects (e.g., car starting). Studies with adults have found similarities and differences in both behavioral and neural responses to cross-modal semantic associations between words and environmental sounds. For example, in a picture detection task, @chen2011crossmodal found a facilitation effect for environmental sounds but not for words when the onset of the auditory stimulus preceded the image by approximately 350 ms. In a follow-up study, @chen2013time presented environmental sounds and words across a wider range of time intervals before the image onset. They found that both naturalistic sounds and spoken words resulted in cross-modal priming, but that the effect of spoken words required more time between the auditory and visual stimuli as compared to the naturalistic sounds. They argue that these data are consistent with a differential processing account: that the recognition of environmental sounds is faster because words must also be processed at a lexical stage before accessing semantic representations, whereas environmental sounds activate semantic representations directly.  

In contrast, other studies have found an advantage for the processing of lexical items compared to environmental sounds. For example, in a sound-to-picture matching task, @lupyan2012evocative showed that words ("cat") lead to faster and more accurate object recognition as compared to either nonverbal cues (the sound of a cat meowing) or lexicalized versions of the nonverbal cue (the word "meowing"). Recent work by @edmiston2015makes followed up on this result by manipulating the congruency between the environmental sounds and their corresponding images within the same basic-level category (e.g., pairing the sound of an acoustic guitar with an image of either an acoustic or an electric guitar). Adults were faster to identify a congruent sound-image pair, suggesting that the environmental sound carried additional information about the specific type of object generating that sound. Critically, adults were fastest identify the images after hearing their labels. @edmiston2015makes argue that this lexical processing advantage is driven by the specificity of the conceptual representations that labels evoke. That is, words function as "unmotivated cues" that are decoupled from surface-level features of a particular instantiation of a category and are therefore better able to evoke abstract category representations that are more useful in distinguishing between categories.

In an ERP study with adults, @cummings2006auditory found that largely overlapping neural networks processed verbal and non-verbal meaningful sounds. In another study focusing on three different sound types that varied in arbitrariness, @hashimoto2006neural found different neural mechanisms for the processing of animal names and vocalizations, with onomatopoeic words activating both areas. Because research on environmental sounds is relatively new, it is hard to reconcile these somewhat discrepant findings. Variations in tasks or timing of stimuli could influence results, and different theoretical commitments can lead to different interpretations. For example, environmental sounds are often treated as encompassing both the sounds of living and human-made objects (e.g., cow mooing, bell ringing) despite evidence that these sounds are treated differently by the adult brain [@murray2006rapid]. Nevertheless, this line of research provides promising new ways to examine the question of whether language emerges from the interaction of domain-general cognitive processes or domain-specific mechanism [@bates1989functionalism]. For example, recent research comparing the processing of speech and non-speech sounds is leading to new insights relevant to autism, developmental language impairment, and cochlear implants [@cummings2010verbal; @mccleery2010neural].

From a developmental perspective, it is also important to understand how children process words and non-arbitrary non-linguistic sounds. However, few studies have examined this question. Using a preferential-looking paradigm, @cummings2009infants found that 15- and 25-mo-olds can use words and environmental sounds to guide their attention to familiar objects, improving as they get older. @vouloumanos2009five found that 5-month-olds can match some animals to the vocalizations they produce. And studies with children with autism and developmental language impairment found more severe deficits for the processing of words than environmental sounds [@cummings2010verbal; @mccleery2010neural].

In the work reported there, we build on these earlier studies using the looking-while-listening paradigm, which has been widely used to assess real-time interpretation of spoken words by infants and young children [@fernald2008looking]. One major goal of this research is to investigate how children process different types of auditory stimuli that are consistently associated with familiar animals but vary in level of arbitrariness. First, we ask whether 32-month-olds can use onomatopoeic sounds (e.g., bow-wow), and animal vocalizations (e.g., dog barking) to identify familiar animals, as well as familiar animal names (e.g., dog). By using real-time processing measures, we can determine whether these three sounds are equally effective as acoustic cues in guiding children's attention to a particular animal in the visual scene. The use of looking to visual stimuli, rather than object-choice responses, reduces the task demands of procedures requiring more complex responses such as reaching or pointing and yield continuous rather than categorical measures of attention on every trial, capturing differences in processing that might not be detected by offline tasks.

A second major goal of this research is to investigate how young children learn to link non-linguistic sounds to animate objects. Do two-year-olds make similar inferences when mapping a novel word and a novel vocalization to an unfamiliar animal?  Typically, word learning is portrayed as an intractable challenge, while associating animals with the sounds they produce might appear trivial. The acoustic structure of vocalizations is influenced by the size and shape of the vocal tract and other physical features, linking sounds to their source in a non-arbitrary way.  And the fact that many animal vocalizations are accompanied by synchronous physical movements might provide children with additional non-arbitrary cues to the source of the sound. Even in the absence of additional visual cues, it is often possible to pinpoint the source of a sound with reasonable accuracy. In contrast, because the acoustic structure of a word is in most cases arbitrary concerning potential referents, and it is produced by a speaker and not by the object itself, learning to associate speech sounds with objects is often characterized as a complex problem of induction [@markman1991whole]. 

To solve the word-learning puzzle, children are said to use constraints on the possible meanings of words. The most widely studied of these constraints is that each object must have only one name [@markman1991whole]. Evidence for this default assumption comes from disambiguation tasks in which children hear a novel label in the presence of a novel object and one or more familiar objects. In these situations, children tend to select the novel object as the referent for the novel word, presumably because the familiar objects already have names associated with them. The debate about the origins, scope, and generality of the Mutual Exclusivity (ME) constraint has focused on whether this response bias provides evidence for a lexical constraint, or whether it results instead from inferences about speakers’ communicative intent. Lexical accounts characterize ME as a "domain specific mechanism specific to word leaning" [@de2011mutual], which "predicts disambiguation only within the domain of word learning (i.e., it is domain-specific)" [@scofield2007two]. Pragmatic accounts propose that ME extends to communicative acts more broadly, reflecting assumptions that speakers are cooperative and should use conventional names to refer to familiar objects [@bloom2002children; @clark1990pragmatics]. A third possibility is that the bias toward one-to-one mappings reflects general tendencies to find simple regularities in complex domains, a perspective embraced by recent computational approaches to word learning [@frank2009using; @mcmurray2012word; @regier2003emergent]. 

Thus, lexical and pragmatic accounts of the scope of the ME constraint predict that one-to-one biases are either unique to word learning or that they generalize to communicative acts more broadly, while domain-general accounts predict that they would apply to any domain in which consistent one-to-one mappings are observed. To explore the possibility that one-to-one biases in sound-object mappings are not limited to interpreting communicative acts, we investigated whether children would show responses comparable to the mutual exclusivity bias in a domain that is neither linguistic nor communicative, but in which consistent associations are observed between objects and auditory cues. 

# Experiment 1

Experiment 1 asks two questions. First, can 32-month-olds use familiar animal names (e.g., dog), onomatopoeic words (e.g., bow-wow), and animal vocalizations (e.g., dog barking) to identify familiar animals? These three sound types differ in arbitrariness along a continuum, with speech as the most arbitrary and vocalizations as the least arbitrary cue. We ask whether these sounds are equally effective as acoustic cues in guiding children's attention to animals in a visual scene. Children will hear a sound cue while looking at images of two familiar animals, one that matches and one that does not match the cue, and we will compare children's looking to the matching animal when hearing the target sound. 

One of three patterns of results is most likely to emerge: The first is that children are faster to identify animal names than onomatopoeic words, and faster to identify onomatopoeic sounds than animal vocalizations. This pattern of results could be predicted by computational models that use frequency as a crucial determinant of speed of processing (e.g., @mcmurray2012word). That is, children in urban environments are more likely to hear the names of animals than their vocalizations, resulting in more practice interpreting speech (high SES children might hear thousands of words daily, but not nearly as many animal sounds). This pattern of results could also be predicted by developmental accounts privilege language cues early development -- either for getting children's attention  [@vouloumanos2004tuned; @vouloumanos2007listening; @vouloumanos2007voice] or for the fact that words refer to objects directly [@waxman2009early] and might evoke more category-diagnostic features compared to environmental sounds [@edmiston2015makes]. 

The second possible pattern of results is that children are faster to identify animal vocalizations than onomatopoeic sounds, and faster to identify onomatopoeic sounds than animal names. This pattern of results could be predicted by accounts that propose that non-arbitrary sounds link directly to semantic representations, while words first activate lexical representations before reaching semantics [@chen2011crossmodal; @chen2013time]. The fact that children have experience interpreting environmental sounds (e.g., balls bouncing, things falling) before learning to interpret speech referentially could also predict an advantage for environmental sounds. A third possibility is that children are equally efficient in exploiting these three sound types to guide their attention to a familiar animal. This pattern of results would parallel that of previous studies that showed little difference in the processing of environmental sounds and words [@cummings2009infants].

Our second question is whether two-year-olds make similar inferences when mapping a novel name and a novel animal vocalization to an unfamiliar animal. In a mutual exclusivity task, children will hear a novel animal name or novel animal vocalization (instead of a familiar one), while looking at the picture of a familiar and a novel animal (instead of two familiar objects). We compare children's proportion of looking to the novel animal when hearing one of these two sound cues. Considering dozens of studies on children's disambiguation biases, we predict that children will look at a novel animal when hearing a novel name. Thus, one of two patterns of results is most likely to emerge: The first is that children look at a novel animal when hearing a novel name, but are show no looking preference when hearing a novel animal vocalization. This pattern of result would be compatible with lexical accounts that predict disambiguation only within the domain of word learning, or by pragmatic accounts that predict disambiguation only within communicative contexts. The second pattern of findings is that children look at a novel animal when hearing a novel name and when hearing a novel animal vocalization, with comparable performance across these two conditions. This pattern would be compatible with accounts that propose that disambiguation biases emerge from domain-general learning mechanisms that look for regularities in complex input.

## Method

```{r filter-e1}
d_e1 <- filter(d, experiment == "e2")
```

```{r make-e1-demo-table}
participants_e1 <- d_e1 %>% 
  select(Sub.Num, CDIAge, Sex, Months) %>% 
  unique() %>% 
  group_by(Sex) %>% 
  summarise(n = n(),
            m_age = mean(Months) %>% round(digits = 2))
```

### Participants

Participants were `r participants_e1$n %>% sum()` 32-month-old children (M=`r mean(d_e1$Months)`; range = `r range(d_e1$Months)[1]`,`r range(d_e1$Months)[2]`, `r participants_e1$n[1]` girls. All were reported by parents to be typically developing and from families where English was the dominant language. Two participants were excluded due to fussiness. Children were from mid/high-SES families.

```{r stimuli-e1, fig.pos="tb", out.width="80%", fig.align="center", fig.cap = "Trial types in Experiments 1 and 2 organized by type of cue: Familiar vs. Novel. The target animal for each trial type is on the left."}

png::readPNG(here("paper/figs", "stimuli.png")) %>% grid::grid.raster()
```

### Visual stimuli 

The visual stimuli included pictures of four Familiar animals (horse, dog, cow, sheep) and two Novel animals (pangolin, tapir).  According to parental report, the familiar animals were known by all children. Parents also reported that the novel animals were completely unfamiliar to the children. Each animal picture was centered on a grey background in a 640 x 480 pixel space

### Auditory stimuli

The auditory stimuli consisted of sounds that were either Familiar or Novel to 32-month-olds. Figure 1 serves as a guide to the different sound types. The Familiar sounds were used in Familiar Trials, and consisted of one of three different sounds: names (horse, dog, cow and sheep), onomatopoeic words (neigh, woof-woof, moo and baa), and vocalizations (horse neighing, dog barking, cow mooing and sheep baaing). The Novel sounds were used in Disambiguation Trials, and consisted of one of two types of sounds: names (capa, nadu) and vocalizations (rhino grunting, gorilla snorting).

Trials in which the auditory cue was a familiar or novel animal name (e.g., Where’s the dog?) or a familiar or novel lexical sound (Which one goes woof-woof?) began with a brief carrier frame. The duration of the target cue was 810 ms for lexical sounds and 750 ms for animal names. The intensity of the phrases was normalized using Praat speech analysis software (Boersma, 2002).

Trials with familiar or novel animal vocalizations began with a single word, used to draw children’s attention (e.g., Look! “dog barking”). Familiar animal vocalizations were selected based on prototypicality. After selecting at least three vocalizations for each familiar animal, the authors voted on the one that we thought would be most easily recognized by children. Choosing the novel animal vocalizations was more challenging. A group of research assistants selected from different websites several vocalizations that they judged as unfamiliar. From these vocalizations, we selected two (i.e., rhino grunting and gorilla snorting) that we judged were equally likely to be produced by the 6 familiar and 2 novel animals based on the their size and vocal tract characteristics. These vocalizations were also maximally distinct from each other and from the familiar animal vocalizations and expected to be unfamiliar to children. We counterbalanced the vocalizations that were paired with the two novel animals, in order to control for the possibility that children judged one of the two novel animals as more likely to produce one of the novel vocalization. All children were reported by parents to have had no exposure to the novel animal’s natural vocalizations.  The duration of the target animal vocalizations was 2000 ms. More details about trial types and conditions in Figure 1 will be given in the Procedure section.

### Familiarization books

There were two main reasons for us to want to make sure that children knew the familiar onomatopoeic words and animal vocalizations before we administered our experiment. First, we wanted to make sure that any differences we observed in children's performance within Familiar-Animal Trials was due to processing speed and could not be explained by the fact that children were not familiar with one of the sound types. The looking-while-listening procedure has been shown to capture differences in processing efficiency even when words are considered "known" by offline reaching tasks or parental report  (Fernald et al., 2008). Second, we wanted to make sure that children knew the pairings between familiar vocalizations and animals, a potential prerequisite for success on Disambiguation trials. Since we were working with children from mid/high-SES families growing up in an urban environment, we were particularly concerned that they would not be familiar with many animal vocalizations or onomatopoeic words. To ensure that all children had at least some experience with the familiar animals and familiar auditory cues used in our study, we gave two children’s books to parents, both titled Sounds on the Farm, a week before their visit. Parents were instructed to share each book with their child for 5 to 10 min on at least three days prior to the experiment. The first book consisted of colorful pictures of each familiar animal and text designed to prompt parents to produce each animal’s lexical sound (e.g., Wow, look at all those cows! This cow says moo, moo!). To give children exposure to the natural animal vocalizations, we used a Hear and There book, which contained buttons that children could press to hear the actual noise that each animal produces.

### Procedure 

Since we were interested in detecting differences in processing between sounds that we expected to be familiar to children, we choose to access speed and accuracy in identifying the correct target picture with the looking-while-listening (LWL) procedure (see Fernald, et al, 2008). Previous studies have shown that even when objects are reported by parents as familiar to their children, or when children are at ceiling in offline reaching tasks, these real-time processing measures can capture meaningful differences in processing. These differences correlate to properties of the sound stimuli (e.g., word-frequency) and different aspects of the child's experience (e.g., their age, socioeconomic status, amount of parental talk). Looking-time measures have also been used in Disambiguation tasks with children from different ages, capturing differences in accuracy that relate to children's age and vocabulary size (Bion et al., 2013).  

On each trial, a pair of pictures was presented on the screen for approximately 4 s, with the auditory stimuli starting after 2 s, followed by 1 s of silence. As seen in Figure 1, we have two main trials types, Familiar Trials and Disambiguation trials, paralleling our original two research questions on children's processing of familiar and novel auditory cues.

Within the Familiar Trials, we have three different sub-trials: name, onomatopoeic word, and vocalization. On 8 Name trials, each familiar animal served as the target twice and was paired once with another familiar animal and once with a novel animal. On 8 Onomatopoeic-word trials, each familiar animal served as the target twice. On 16 Vocalization trials, each familiar animal served as the target four times, paired twice with another familiar animal and twice with a novel animal. These three familiar sound types should allow us to answer our first research question, asking whether names, onomatopoeic words, or animal vocalizations, are equally effective as acoustic cues in guiding children's attention to animals in a visual scene.

Within the Disambiguation Trials, we have two different sub-trials: name, and vocalizations. On 6 Name trials, each novel animal was labeled three times with a novel animal name (i.e., capa, nadu), always paired with a familiar animal. On 8 Vocalization trials, each novel animal vocalization served as the target four times and was paired with each familiar animal once. These two sound types should allow us to answer our second research question, asking whether two-year-olds make similar inferences when mapping a novel name and a novel animal vocalization to an unfamiliar animal.

These different trial types were administered in two different visits. The Familiar and Disambiguation Trials with animal names and onomatopoeic words were administered during children’s first visit. The Familiar and Disambiguation Trials with the animal vocalizations were administered during their second visit. We administered the animal vocalizations on the second visit to allow children to become familiar with the procedure and to give parents additional time to use the familiarization books with the vocalizations with their children. During each visit, five Filler trials were interspersed throughout to add variety and maintain children’s attention. Pairings of the novel animal and name, and side of presentation of target animals, were counterbalanced across participants. Caregivers wore darkened sunglasses so that they could not see the pictures and influence infants’ looking throughout the 5-min procedure.

### Measures of processing efficiency

Participants' eye movements were video-recorded and coded with a precision of 33 ms by observers who were blind to trial type. Inter- and intra-observer reliability checks were conducted for all coders. For 25% of the subjects, two measures of inter-observer reliability were assessed. The first was the proportion of frames (33-ms units) on each trial on which two coders agreed. In this case, agreement was 98%. However, because this analysis included many frames on which the child was maintaining fixation on one picture, we also calculated a more stringent test of reliability. This second measure focused only on shifts in gaze, ignoring steady-state fixations in each trial on which agreement was inevitably high. By this more conservative measure, coders agreed within one frame on 94% of all shifts.

**Accuracy:** On those trials in which the infant was fixating a picture at the onset of the speech stimulus, accuracy was computed by dividing the time looking to the target object by the time looking to both target and distracter, from 300 to 2500 ms from the onset of the target word. Accuracy before 300 ms was not included because shifts to the target occurring in this window had presumably been initiated before the onset of the noun. This analyses window was chosen because of the longer duration of the animal vocalizations (2 s.) and because of the introduction of novel auditory cues. A single analyses window was used for all trial types for consistency. Mean accuracy was then computed for each participant on each trial type.

**Reaction time:** We calculated reaction time (RT) on those trials on which participants were looking at the distractor animal at the beginning of the sound. RT on each trial was the latency of the first shift to the correct animal within a 300- to 1,800-ms window from sound onset, as typically done in studies using this procedure [@fernald2008looking].

## Results and discussion

### Familiar Trials: Using familiar animal names, onomatopeic sounds, and animal vocalization to identify familiar animals:

Our first question is whether a familiar animal name, onomatopoeic word, and animal vocalization are equally effective in guiding children's attention to an animal in the visual scene. Figure 2A shows children's looking behavior over time on the LWL procedure [@fernald1998rapid]. In order to capture children's speed of processing, we show children's responses on trials in which they start looking at the wrong animal. From sound onset onward, we show the mean proportion of trials in which children were looking at the correct picture, every 33ms, with different lines representing children's responses on Name trials (black), Onomatopoeic-word trials (light grey), and Vocalization trials (dark grey). The y-axis shows the mean proportion of trials on which children were looking at the correct animal. The x-axis represents time from sound onset in milliseconds. Around 750ms from sound onset there is already a substantial difference in the mean proportion of trials in which children are looking at the correct animal depending on whether they heard an animal name or an animal vocalization. Children are looking at the correct animal in a greater proportion of trials when they hear a familiar animal name, as compared to when they hear an animal vocalization, with performance on trials with onomatopoeic words falling between the two.

```{r filter-model-rt-e1}
d_model_e1_rt <- d_models$rt_e1 
```

```{r e1-rt-model-sumamry}
e1_model_summary_rt <- d_model_e1_rt %>% 
  group_by(cue_type) %>% 
  summarise(m = mean(rt_scale_param),
            hdi_lower = quantile(rt_scale_param, probs = 0.025),
            hdi_upper = quantile(rt_scale_param, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2)
```

```{r e1-rt-group-diffs}
e1_rt_hyp_d <- d_model_e1_rt %>% 
  select(-param_est) %>% 
  spread(key = cue_type, value = rt_scale_param) %>% 
  mutate(name_onomatopoeic_diff = name - onomatopoeia,
         name_vocalization_diff = name - vocalization,
         onomatopeic_vocalization_diff = onomatopoeia - vocalization) %>% 
  select(-name, -onomatopoeia, -vocalization) %>% 
  gather(key = ind_contrast, value = param_est, -sample_id) 

e1_rt_hyp_table <- e1_rt_hyp_d %>% 
  group_by(ind_contrast) %>% 
  summarise(m = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2)

prob_diff0 <- e1_rt_hyp_d %>% 
  group_by(ind_contrast) %>% 
  summarise(prob = mean(param_est < 0)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 4)
```

To quantify these differences, we fit Bayesian mixed-effects regression models using the `rstanarm` [@gabry2016rstanarm] package in `r my_citations$r`. The mixed-effects approach allowed us to model the nested structure in our data by including random intercepts for each participant and item and a random slope for each item. We used Bayesian methods in order to quantify support in favor of null hypotheses of interest – in this case, the absence of a difference in real-time processing across the different familiar cue types. To communicate the uncertainty in our estimates, we report the 95% Highest Density Interval (HDI) around the point estimates of the group means and the difference in means. The HDI provides a range of plausible parameter values given the data and the model. All analysis code can be found in the online repository for this project: https://github.com/kemacdonald/anime.

[^papaja_pkg_citations]: `r my_citations$pkgs`

We computed reaction time (RT) as the mean time it took them to shift to the correct picture on trials in which they were looking at the wrong picture at sound onset for the three cue types. To make RTs more suitable for modeling on a linear scale, we analyzed responses in log space using a logistic transformation, with the final model was specified as: $log(RT) \sim cue\_type + (1 + sub\_id \mid item)$. 

Figure 2 shows the data distribution for each participant's RT, the estimates of condition means, and the full posterior distribution of condition differences across the different cue types. Children were faster to identify the target animal while hearing its name ($M_{name}$ = `r e1_model_summary_rt$m[3]` ms), as compared to its onomatopeic animal sound ($M_{onomatopeia}$ = `r e1_model_summary_rt$m[1]` ms), and its vocalization ($M_{vocalization}$ = `r e1_model_summary_rt$m[2]` ms). The difference between RTs for the name and onomatopeic animal sounds was `r e1_rt_hyp_table$m[1]` ms with a HDI from `r e1_rt_hyp_table$hdi_lower[1]` ms to `r e1_rt_hyp_table$hdi_upper[1]` ms. While the null value of zero difference falls within the 95% HDI, `r prob_diff0$prob[1] * 100`% of the credible values fall below the null, providing some evidence for faster processing of animal names. The average difference in children's RT between the name and vocalization trials was `r e1_rt_hyp_table$m[2]` ms with a HDI from `r e1_rt_hyp_table$hdi_lower[2]` ms to `r e1_rt_hyp_table$hdi_upper[2]` ms and `r prob_diff0$prob[2] * 100`% of the credible values falling below zero, providing strong evidence that children processed names more efficiently compared to vocalizations. Finally, the average difference in children's RT between the onomatopeic sounds and vocalization trials was `r e1_rt_hyp_table$m[3]` ms with a HDI from `r e1_rt_hyp_table$hdi_lower[3]` ms to `r e1_rt_hyp_table$hdi_upper[3]` ms and `r prob_diff0$prob[3] * 100`% of the credible values falling below zero. 

Together, the RT modeling results provide strong evidence that children processed animal names around `r  e1_rt_hyp_table$m[2] %>% round(digits = 0) %>% abs()` ms faster than animal vocalizations, with almost all of the estimates of the plausible RT differences falling below the null value of zero. There was slightly weaker evidence that children processed animal names more efficiently compared to onomatopeic animal sounds but strong evidence of faster processing of onomatopeic animal sounds compared to animal vocalizations. In sum, there was evidence of a graded effect of cue type on RTs with names being faster than onomatopeic animal sounds, which were faster than animal vocalizations.

```{r oc-plot-e1, fig.pos="t", fig.align='center', out.width = "80%", fig.cap="Reaction Time (RT) results for Experiment 1. Panel A shows the timecourse of children’s looking to the target animal after hearing a familiar animal name (black), onomatopoeic word (light grey), or animal vocalization (dark grey). Panel B shows the distribution of RT data across conditions. Each grey point shows a RT for a single trial. The black bar represent the most likely estimate of the condition means. The red lines connect the condition means to illustrate shifts in the RT distributions. Panel C shows the posterior distribution of credible RT differences across conditions. Color and linetype represent the contrast of interest and the red vertical dashed line represents the null value of zero condition difference. All error bars represent 95\\% Highest Density Intervals."}

png::readPNG(here("paper/figs", "oc_e1_new.png")) %>% grid::grid.raster()
```

```{r filter-model-acc-e1}
d_model_acc_e1 <- d_models$acc_e1 %>% 
  mutate(condition = str_c(cue_type, trial_type, sep = "_")) %>% 
  filter(condition != "onomatopoeic_novel") 
```

```{r summarise-model-acc-e1}
# summarize posterior distributions for accuracy
e1_model_summary_acc <- d_model_acc_e1 %>% 
  group_by(cue_type, trial_type) %>% 
  summarise(m = mean(acc_prob_scale),
            hdi_lower = quantile(acc_prob_scale, probs = 0.025),
            hdi_upper = quantile(acc_prob_scale, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) 
```

```{r summarise-group-comparisons-acc-fam-e1}
e1_acc_hyp_fam_d <- d_model_acc_e1 %>% 
  filter(trial_type == "familiar") %>% 
  select(-param_estimate, -condition) %>% 
  spread(key = cue_type, value = acc_prob_scale) %>% 
  mutate(name_onomatopoeic_diff = onomatopoeic - name,
         name_vocalization_diff = vocalization - name,
         onomatopeic_vocalization_diff = vocalization - onomatopoeic) %>% 
  select(ends_with(match = "diff"), sample_id) %>% 
  gather(key = ind_contrast, value = param_est, -sample_id) 

e1_acc_hyp_fam_table <- e1_acc_hyp_fam_d %>% 
  group_by(ind_contrast) %>% 
  summarise(m = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2)

null_value_acc_diff <- 0
null_value_acc_chance <- 0.5

prob_diff_chance_e1_acc_fam <- e1_acc_hyp_fam_d %>% 
  group_by(ind_contrast) %>% 
  summarise(prob = mean(param_est < null_value_acc_chance)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 4)
```

Children's reaction times showed a difference in speed of processing for names, onomatopoeic words, and vocalization. Next, we estimated children's attention to the target image over the course of the trial to ask whether the three trial types were equally effective cues to guide their attention to a familiar animal. We computed accuracy over a window from 300 to 2500 ms after the onset of the cue. The upper left panel of Figure 3A shows children's proportion looking to the target for each trial type. Each point represents a single participant's Accuracy, the grey line shows the full distribution of the data, and the horizontal line shows the median value. The orange points represent the most likely estimate for the mean proportion looking with the error bars showing the 95% HDI. Visual inspection of the plot suggests two things: (1) children reliably looked to the correct animal after hearing each of the three familiar cues and (2) children's overall looking behavior was strikingly similar across conditions.

Next, we quantified the strength of evidence for the absence of any condition differences and for the difference from random responding. We estimated the mean proportion looking for each trial type using a Bayesian linear mixed-effects model with the same specifications as the RT model described above. We transformed the proprotion looking scores using the empirical logit, with the final model as: $logit(accuracy) \sim cue\_type + (1 + sub\_id \mid item)$. The orange points in Figure 3A show mean Accuracy for each familiar cue type ($M_{name}$ = `r e1_model_summary_acc$m[1]`, $M_{onomatopeia}$ = `r e1_model_summary_acc$m[3]`, and ($M_{vocalization}$ = `r e1_model_summary_acc$m[4]`). Children's looking to the target image was reliably different from a model of random looking behavior across all conditions, with the null value of 0.5 falling well outside of the range of plausible values (see the difference between the horizontal dashed line and error bars in Figure 3A). 

Moreover, the three cues types were equally effective in guiding children’s attention to the target animal over the course of the trial as shown by the high overlap in the posterior distributions of Accuracy and that the null value of zero difference fell within the HDI for all group comparisons (name vs. onomatopeia: $\beta_{diff}$ = `r e1_acc_hyp_fam_table$m[1]`, 95% HDI from `r e1_acc_hyp_fam_table$hdi_lower[1]` to `r e1_acc_hyp_fam_table$hdi_upper[1]`; name vs. vocalization: $\beta_{diff}$ = `r e1_acc_hyp_fam_table$m[2]`, 95% HDI from `r e1_acc_hyp_fam_table$hdi_lower[2]` to `r e1_acc_hyp_fam_table$hdi_upper[2]`; onomatopeia vs. vocalization: $\beta_{diff}$ = `r e1_acc_hyp_fam_table$m[3]`, 95% HDI from `r e1_acc_hyp_fam_table$hdi_lower[3]` to `r e1_acc_hyp_fam_table$hdi_upper[3]`). These results provide evidence that the animal vocalizations were equally effective at directing overall looking behavior to identify familiar animals as their names or onomatopeic sounds if children have enough time to process the cue.

### Disambiguation Trials: Using novel animal names and animal vocalization to disambiguate novel animals

```{r summarise-model-acc-e1-novel}
e1_acc_hyp_nov_d <- d_model_acc_e1 %>% 
  filter(trial_type == "novel") %>% 
  select(-param_estimate, -condition) %>% 
  spread(key = cue_type, value = acc_prob_scale) %>% 
  mutate(name_vocalization_diff = vocalization - name) %>% 
  select(ends_with(match = "diff"), sample_id) %>% 
  gather(key = ind_contrast, value = param_est, -sample_id) 

e1_acc_hyp_nov_table <- e1_acc_hyp_nov_d %>% 
  group_by(ind_contrast) %>% 
  summarise(m = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2)

null_value_acc_diff <- 0
null_value_acc_chance <- 0.5

prob_diff_chance_e1_acc_nov <- e1_acc_hyp_nov_d %>% 
  group_by(ind_contrast) %>% 
  summarise(prob = mean(param_est < null_value_acc_chance)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 4)
```

Our next question is whether children would orient to a novel animal after hearing a novel animal name or vocalization, thus showing evidence of one-to-one biases for the vocalizations that animals produce. We focus on children's accuracy, comparing their proportion of looking to the novel animal against chance performance and across cue types.  The orange points in Figure 3A show Accuracy group means for each novel cue type ($M_{name}$ = `r e1_model_summary_acc$m[2]`, $M_{vocalization}$ = `r e1_model_summary_acc$m[5]`). Children's looking to the target image was reliably different from a model of random looking behavior across both cue types, with the null value of $0.5$ falling well outside of the range of plausible values. 

Moreover, the novel animal name and animal vocalizations were equally effective in guiding children’s attention to the target animal over the course of the trial (name vs. onomatopeia: $\beta_{diff}$ = `r e1_acc_hyp_nov_table$m[1]`, 95% HDI from `r e1_acc_hyp_nov_table$hdi_lower[1]` to `r e1_acc_hyp_nov_table$hdi_upper[1]`). This result provides strong evidence that the animal vocalizations were equally effective at directing overall looking behavior to identify familiar animals as their names or onomatopeic sounds if children have enough time to process the cue.

```{r acc-plot-e1, fig.pos="t", fig.align='center', out.width = "80%", fig.width=4, fig.cap = "Accuracy results for Experiment 1 for familiar (upper panels) and novel (lower panels) trials. Panel A shows the data distribution and model estimates for Accuracy of children's looking behavior. Panel B shows the full posterior distribution over model estimates of differences in accuracy across conditions. The vertical dashed line represents the null model of zero difference in Accuracy. All other plotting conventions are the same as in Figure 2."}

png::readPNG(here("paper/figs", "prop_look_e1_new.png")) %>% grid::grid.raster()
```

Therefore, children seem to have one-to-one biases for the vocalizations that animals produce already at 32 months of age, the earliest age at which the disambiguation effect has been observed in a domain other than word learning. There were no significant differences between children's reaction time for novel animal names or vocalizations.

# Experiment 2

One issue that has received much attention in recent years concerns the relation between children’s referent selection and retention abilities. While earlier studies tended to conflate disambiguation strategies and children’s word learning, more recent studies suggest that these two abilities should not be conflated [@bion2013fast; @horst2008fast]

@horst2008fast examined both referent selection and retention in four experiments with 2-year-olds.  When children were shown a novel object among familiar objects, they selected the novel object when hearing a novel label, as found in previous studies.  But surprisingly, on retention trials 5 min later, these children showed no evidence of remembering the names of the novel objects they had previously identified. Using a looking-time task, @bion2013fast replicated these findings in a study with 18-, 24-, and 30-month-old infants using looking time measures of performance.

Experiment 2 asks whether children can retain the link created through disambiguation between a novel animal and a novel animal vocalization. We also aim to replicate the findings from Experiment 1, showing that children can identify familiar animals based on the vocalizations they produce and use novel vocalizations to disambiguate novel animals. We predict that children will succeed in disambiguation trials, but will show little evidence of retention on subsequent disambiguation trials, paralleling the findings of earlier studies with linguistic stimuli.

## Method

### Participants

```{r participant-demo-e2}
d_e2 <- filter(d, experiment == "e2")

participants_e2 <- d_e2 %>% 
  select(Sub.Num, CDIAge, Sex, Months) %>% 
  unique() %>% 
  group_by(Sex) %>% 
  summarise(n = n(),
            m_age = mean(Months) %>% round(digits = 2))
```

Participants were `r sum(participants_e2$n)` 31-month-old children (M = `r d_e2$Months %>% mean(na.rm=T)` months; range = `r d_e2$Months %>% min()` - `r d_e2$Months %>% max()`), `r participants_e2$n[1]` girls. All children were typically developing and from families where English was the dominant language.

### Visual stimuli  

The visual stimuli were the same as in Experiment 1, except for the novel animals (aardvark and capybara), which replaced the novel animals (pangolin and tapir) used in Experiment 1 (see animals in Figure 4). We decided to change the novel animals in order to confirm that our results were not restricted to the particular stimuli set in Experiment 1. All children were reported by parents to have had no exposure to the novel animals.

### Auditory stimuli  

The auditory stimuli consisted of the same familiar and novel animal vocalizations as in Experiment 1.

```{r stimuli-e2, fig.pos="t", out.width="80%", fig.align="center", fig.cap = "Trial types in Experiments 4 organized by type of trial. Children hear familiar and novel vocalizations. The target animal for each trial type is on the left."}

png::readPNG("figs/stimuli_e2.png") %>% grid::grid.raster()
```

### Familiarization books 

As in Experiment 1, we sent home a children’s book to ensure that all participants had at least some exposure to the familiar animals and auditory cues. Since, in Experiment 2, we were interested in the natural animal vocalizations and not the names/lexical sounds, only the Hear and There Sounds on the Farm book was used.  Instructions given to the parents were the same as in Experiment 1, and the book was sent home a week before the visit.

### Procedure

Experiment 2 consisted of one visit. Each child saw 30 trials, consisting of three trial types (Figure 4). The 16 Familiar trials and 8 Disambiguation trials were identical in structure to the Vocalization trials Experiment 1. In addition, on 6 Retention trials, the two novel animals were presented side by side, with each serving as the target three times.  The same coding and speed/accuracy measures were used as in Experiment 1.

## Results and discussion

```{r e2-model-filter}
d_model_acc_e2 <- d_models$acc_e2 
```

### Retention of the link between a novel animal and a novel vocalization:

```{r e2-acc-model-summary}
# summarize posterior distributions for accuracy
e2_model_summary_acc <- d_model_acc_e2 %>% 
  group_by(trial_type) %>% 
  summarise(m = mean(acc_prob_scale),
            hdi_lower = quantile(acc_prob_scale, probs = 0.025),
            hdi_upper = quantile(acc_prob_scale, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2) 
```

```{r e2-acc-group-comparison}
e2_acc_hyp_d <- d_model_acc_e2 %>%
  select(-param_estimate) %>% 
  spread(key = trial_type, value = acc_prob_scale) %>% 
  mutate(fam_disambig_diff = familiar - disambiguation,
         fam_reten_diff = familiar - retention,
         disambig_reten_diff = disambiguation - retention) %>% 
  select(ends_with(match = "diff"), sample_id) %>% 
  gather(key = ind_contrast, value = param_est, -sample_id) 

e2_acc_hyp_table <- e2_acc_hyp_d %>% 
  group_by(ind_contrast) %>% 
  summarise(m = mean(param_est),
            hdi_lower = quantile(param_est, probs = 0.025),
            hdi_upper = quantile(param_est, probs = 0.975)) %>% 
  mutate_if(.predicate = is.numeric, .funs = round, digits = 2)
```

Figure 5A shows children’s proportion looking to the target animal after hearing a familiar or a novel animal vocalization over the same analysis window used in Experiment 1 (300 to 2500 ms after the onset of the vocalization). Visual inspection of the figure suggests that children successfully oriented to the target image after hearing both familiar and novel animal vocalizations. The orange points show mean proportion target looking for familiar ($M_{familiar}$ =  `r e2_model_summary_acc$m[2]`, disambiguation ($M_{disambiguation}$ =  `r e2_model_summary_acc$m[1]`, and retention trials ($M_{retention}$ = `r e2_model_summary_acc$m[3]`). 

Children's looking to the target image was reliably different from random-looking behavior for both familiar and disambiguation trials, with the null value of `r null_value_acc_chance` falling well outside of the range of plausible values. Moreover, the novel animal vocalizations and the familiar animal vocalizations were equally effective in guiding children’s attention to the target animal over the course of the trial (familiar vs. disambiguation: $\beta_{diff}$ = `r e2_acc_hyp_table$m[2]`, 95% HDI from `r e2_acc_hyp_table$hdi_lower[2]` to `r e2_acc_hyp_table$hdi_upper[2]`). 

In contrast to children's success on familiar trials and disambiguation trials, they did not show evidence of retaining the link between the novel animal and the novel animal vocalization, with the null value of $0.5$ proportion looking falling well within the range of plausible estimates. Moreover, there was strong evidence that children were less accurate on retention trials compared to both disambiguation trials ($\beta_{diff}$ = `r e2_acc_hyp_table$m[1]`, 95% HDI from `r e2_acc_hyp_table$hdi_lower[1]` to `r e2_acc_hyp_table$hdi_upper[1]`) and familiar trials ($\beta_{diff}$ = `r e2_acc_hyp_table$m[3]`, 95% HDI from `r e2_acc_hyp_table$hdi_lower[3]` to `r e2_acc_hyp_table$hdi_upper[3]`).

```{r acc-plot-e2, fig.pos="t", fig.align='center', out.width = "80%", fig.width=5, fig.asp = 0.6,, fig.cap="Accuracy of responses to familiar and novel animal vocalizations in Experiment 2. Panel A shows the data distribution alongside the model estimates of mean Accuracy across the different trial types. Panel B shows the full posterior distribution over model estimates of differences in accuracy across trial types. The vertical dashed line represents the null model of zero difference. All other plotting conventions are the same as in Figures 1 and 2."}

png::readPNG(here("paper/figs", "prop_look_e2_new.png")) %>% grid::grid.raster()
```

Three findings emerged from the accuracy analysis: First, children oriented to a familiar animal after hearing a familiar animal vocalization. Second, children oriented to a novel animal after hearing a novel animal vocalization. These two results are an internal replication of the key findings from Experiment 1 in a new sample. In addition, we found that children did not show evidence of retaining the link between a novel animal vocalization and a novel animal. These results and their implications are discussed in more detail in the following section.

# General Discussion

Three main findings emerged from this work. The first finding was that 30-month-olds responded fastest to a familiar animal name and slowest to a familiar animal vocalization, with onomatopoeic sounds falling somewhere in between. Children could, however, identify the familiar animals after hearing any of on these three sound types. The second finding was that children showed disambiguation biases for the types of vocalizations that animals produce, similar to their biases in word learning. The third finding was that these biases do not necessarily lead to learning, as children were not successful in retaining the link between novel animals and their vocalizations. This lack of retention parallels the findings of recent word-learning studies (see @mcmurray2009integrating) and emphasizes the theoretical importance of disentangling processes of disambiguating reference, an in-the-moment phenomenon, and word learning, which occurs over a longer timescale.

In our study, we found a processing speed advantage for words over other meaningful sounds. Some theories of language development argue that words are unique stimuli because they refer to objects in the world [@waxman2009early], while other theories argue that words are special because they activate conceptual information more quickly, accurately, and in a more categorical way than nonverbal sounds [@edmiston2015makes]. It is also possible that words and nonverbal sounds might be processed by different brain regions, with words being accessed more rapidly. A second explanation for the advantage for words might be differences in sheer frequency in the input. At least in our sample, it is safe to assume that children have heard the word cow many more times than they have heard an actual cow mooing. Frequency effects have been robustly demonstrated in the processing of words, with adults being faster to recognize words that they hear more frequently [@dahan2001time]. A final explanation is that words are very effective at presenting a lot of information in a short period. When children see a simple visual world consisting of a dog and a sheep, the first phoneme of the target word is already sufficient to determine the animal that is likely to be talked about next.

Much less is known about children’s and adults’ processing of onomatopoeic sounds. @hashimoto2006neural compared brain responses to nouns, animal sounds, and onomatopoetic sounds, and found that onomatopoeic sounds were processed by extensive brain regions involved in the processing of both verbal and nonverbal sounds. @cummings2009infants argues that onomatopoeic sounds might provide young children with information about intermodal associations, bridging their understanding of non-arbitrary environmental sounds and arbitrary word-object associations. @fernald1993common reported that 52% of Japanese mothers used onomatopoeic sounds to label target objects, while only 1 in 30 American mothers did so. While our results do not speak directly to these theoretical issues, they do suggest that onomatopoeic sounds function like words in that they are capable of activating conceptual representations that drive children's visual attention to seek the physical referent of the sound. However, there was some evidence that children processed onomatopeic sounds less efficiently compared to words in our task.

Our second finding was that children looked at a novel animal when hearing a novel animal vocalization, with accuracy comparable to their disambiguation of novel animal names. @bloom2002children outlines three different theories that could explain children’s disambiguation biases: These biases could be a specifically lexical phenomenon that applies only to words (lexical account), a product of children’s theory of mind restricted to communicative situations (pragmatic account), or a special case of a general principle of learning that exaggerates regularities across domains (domain-general account). By using animal sounds, this study provides an important data point since the theoretical accounts make different predictions for children's looking behavior in response to a stimulus that is non-linguistic and non-communicative.

Previous studies contrasted lexical-specific and pragmatic accounts. For example, diesendruck2001children found that children expect speakers to use consistent facts to refer to objects, and they select a novel object when hearing a novel fact. Recent studies suggest that different strategies might be used to make inferences about speakers’ communicative intent and the meaning of a novel word. Autistic children who are known to have pragmatic deficits show disambiguation biases and select a novel word when hearing a novel object [@preissler2005role]. Moreover, disambiguation biases for words are correlated with vocabulary, and disambiguation biases for facts are correlated with social-pragmatic skills [@de2011mutual]. These findings suggest that disambiguation biases for words might not be motivated uniquely by pragmatic inferences, but they do not provide evidence for or against domain-general accounts of the disambiguation bias.

Relatively few studies have looked at disambiguation biases in non-linguistic domains. moher2010one showed that three-year-olds link different voices to unique faces, showing that one-to-one biases might extend to other communicative domains. However, @yoshida2012exclusion found that adults in a statistical learning task were less likely to show evidence of using disambiguation biases to learn nonspeech sounds even though this behavior would have facilitated task performance. These results suggest that at some point in development disambiguation constraints may operate more strongly over speech compared to nonspeech sounds. Critically, these results do not provide evidence against a domain-general account since participants had no reason to expect that the mapping between random non-linguistic sounds and objects should be mutually exclusive. It could be that similar learning strategies might be applied to non-linguistic sounds when they become meaningfully related to objects in the environment or relevant for communication with other people.

The work reported here demonstrates that young children do show evidence of disambiguation biases in a non-linguistic and non-communicative domain. These findings support predictions made by domain-general accounts that explain disambiguation biases as the byproduct of a system that attempts to find regularities in complex learning tasks that involve consistent mappings. Previous Connectionist and Bayesian models of word learning showed that disambiguation biases emerge as children are exposed to consistent mappings between words and objects, without built-in constraints on the types of meanings words could have [@frank2009using; @mcmurray2012word; @regier2003emergent]. In principle, these same biases could emerge if these models attempted to map animal vocalizations to animals and there were consistent co-occurrences present in the learning environment.

One open question is whether a single disambiguation mechanism can account for the diverse set of contexts in which children show disambiguation behaviors. Here, we found that children could disambiguate stimuli other than words and facts, suggesting at least the existence of a domain-general mechanism that leads to disambiguation. A preference for parsimony suggests that we should favor of a single mechanism. But as pointed out by recent computational work, it is possible that different mechanisms jointly contribute to disambiguation behavior, explaining findings across different populations and contexts [@lewis2013integrated]. Thus, it is possible that the same behavior - selecting a novel object when hearing a novel auditory stimulus - might result from different computational mechanisms or motivations depending on the task at hand, children's age, or the particular stimuli set and assumptions about the people involved in the interaction. That is, children could use a domain-general mechanism to learn about novel animal vocalizations, and they could use a lexical and pragmatic constraint to learn about novel words.

Our third finding was that one-to-one biases for animal vocalizations do not necessarily lead to retention of the link between a novel animal and a novel vocalization. This finding dovetails with recent cross-situational models of early word learning that emphasize the separate processes of figuring out reference in the moment and learning word-object labels over time [@mcmurray2012word]. McMurray and colleagues propose that referent-selection requires that children give their best guess about a new word's meaning in a specific ambiguous situation, but that learning operates over a much longer timescale, requiring multiple exposures to build up stable word-object links. Although disambiguation can be viewed as the product of learning that has occurred up to that point, for younger children it does not necessarily result in learning. These claims are also supported by evidence from studies on early word learning using online and offline measures of retention [@bion2013fast].

These results also add to a recent body of work that encourages us to think differently about disambiguation biases. This work has emphasized the role of experience, showing that the tendency to select a novel object when hearing a novel word is not robustly present across populations. For example, bilingual children, children from lower socioeconomic status, children who receive less language input, and children with less structured vocabularies or smaller vocabulary sizes, take longer to show evidence of disambiguation biases [@bion2013fast; @yurovsky2012mutual]. Other studies have problematized the relation between disambiguation biases and word learning, showing that success in referent selection does not necessarily mean that the link between the novel word and novel object will be retained  [@bion2013fast; @horst2006online; @mcmurray2012word]. And the current study adds to recent studies taking a fresh look at an old question: the scope of disambiguation biases [@suanda2012detailed; @suanda2013young]. Taken together, these findings suggest that it is important to consider variability in the emergence, function, and scope of any language learning processes that might be characterized as universal based on studies using a particular population or a specific stimulus type.

Finally, our results emphasize that children’s learning about objects in their environment involves more than learning their names. Before object names are learned, sounds and actions might form the basis on which objects are conceptualized. For example, children might see barking as a defining feature of dogs and may say bow-wow in response to the picture of a dog, even before they learn the animal name [@nelson1974concept]. Learning the meaning of an object, therefore, requires learning several cross-modal associations, including learning the object’s texture, smell, as well as its sounds and names. Children do not have explicit constraints that freshly baked cookies should have only one smell. Yet, they might recognize and get excited about the familiar smell coming from the kitchen and might assume their mothers are baking something new when smelling something unfamiliar.

## Conclusions

Children use different types of knowledge to make sense of a constantly changing world. They might identify animal vocalizations based on the shape of the vocal tract of the animal, its location and size, and their previous knowledge about animal vocalizations. Importantly, these cues normally converge in helping children identify an animal in the environment. The same is true for their identification of referents for words. Children can identify the referent for a word based on semantics [@goodman1998role], cross-situational statistics [@smith2008infants], syntax [@brown1957linguistic], and pragmatic and social cues [@baldwin1993infants], and disambiguation biases [markman1991whole]. As children grow older, these different sources of information provide converging evidence that a novel word should refer to a novel object. Children can rely on their knowledge about the world, speakers, and on their previous experiences with words to figure out what speakers are talking about – a task we continue to do throughout our lives when learning new words and interpreting complex sentences.


\newpage

# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}


